{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1hkpjKL8h2b"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
        "new_names = ['sepal_length','sepal_width','petal_length','petal_width','iris_class']\n",
        "dataset = pd.read_csv(url, names=new_names, skiprows=0, delimiter=',')\n",
        "dataset.info()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kppjdnpP8lBI",
        "outputId": "b193ff2f-32f3-45d8-e0ee-343f25866c9d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 150 entries, 0 to 149\n",
            "Data columns (total 5 columns):\n",
            " #   Column        Non-Null Count  Dtype  \n",
            "---  ------        --------------  -----  \n",
            " 0   sepal_length  150 non-null    float64\n",
            " 1   sepal_width   150 non-null    float64\n",
            " 2   petal_length  150 non-null    float64\n",
            " 3   petal_width   150 non-null    float64\n",
            " 4   iris_class    150 non-null    object \n",
            "dtypes: float64(4), object(1)\n",
            "memory usage: 6.0+ KB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.head(6)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 237
        },
        "id": "b4RofQCT86Zp",
        "outputId": "4b2dae37-113e-48cb-aaf6-7b8edbfe7742"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "   sepal_length  sepal_width  petal_length  petal_width   iris_class\n",
              "0           5.1          3.5           1.4          0.2  Iris-setosa\n",
              "1           4.9          3.0           1.4          0.2  Iris-setosa\n",
              "2           4.7          3.2           1.3          0.2  Iris-setosa\n",
              "3           4.6          3.1           1.5          0.2  Iris-setosa\n",
              "4           5.0          3.6           1.4          0.2  Iris-setosa\n",
              "5           5.4          3.9           1.7          0.4  Iris-setosa"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-d12d5244-8c03-4933-a176-eea403317f55\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sepal_length</th>\n",
              "      <th>sepal_width</th>\n",
              "      <th>petal_length</th>\n",
              "      <th>petal_width</th>\n",
              "      <th>iris_class</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>5.1</td>\n",
              "      <td>3.5</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4.9</td>\n",
              "      <td>3.0</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>4.7</td>\n",
              "      <td>3.2</td>\n",
              "      <td>1.3</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>4.6</td>\n",
              "      <td>3.1</td>\n",
              "      <td>1.5</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>5.0</td>\n",
              "      <td>3.6</td>\n",
              "      <td>1.4</td>\n",
              "      <td>0.2</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>5.4</td>\n",
              "      <td>3.9</td>\n",
              "      <td>1.7</td>\n",
              "      <td>0.4</td>\n",
              "      <td>Iris-setosa</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-d12d5244-8c03-4933-a176-eea403317f55')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-d12d5244-8c03-4933-a176-eea403317f55 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-d12d5244-8c03-4933-a176-eea403317f55');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset.iris_class.value_counts()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qo_vFc4eJvoH",
        "outputId": "f969e381-daf2-4cec-fcab-3881668c7cee"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Iris-setosa        50\n",
              "Iris-versicolor    50\n",
              "Iris-virginica     50\n",
              "Name: iris_class, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "y = dataset['iris_class']\n",
        "x = dataset.drop(['iris_class'], axis=1)\n",
        "\n",
        "print (\"dataset : \",dataset.shape)\n",
        "print (\"x : \",x.shape)\n",
        "print (\"y : \",y.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fBvD7hSC8-P-",
        "outputId": "d36ac27d-1333-4398-b4c1-0c7ac6ed2ad1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "dataset :  (150, 5)\n",
            "x :  (150, 4)\n",
            "y :  (150,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#one hot encoding\n",
        "y=pd.get_dummies(y)\n",
        "y.sample(7)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 269
        },
        "id": "5hQzAgHf9ABZ",
        "outputId": "fd6b63e1-b199-4cf7-98d2-064310c0bba5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "     Iris-setosa  Iris-versicolor  Iris-virginica\n",
              "37             1                0               0\n",
              "95             0                1               0\n",
              "91             0                1               0\n",
              "2              1                0               0\n",
              "81             0                1               0\n",
              "147            0                0               1\n",
              "96             0                1               0"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-addcdd2f-46e0-4f76-bb86-fe7b8eb4af76\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Iris-setosa</th>\n",
              "      <th>Iris-versicolor</th>\n",
              "      <th>Iris-virginica</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>37</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>95</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>91</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>81</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>147</th>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>96</th>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-addcdd2f-46e0-4f76-bb86-fe7b8eb4af76')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-addcdd2f-46e0-4f76-bb86-fe7b8eb4af76 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-addcdd2f-46e0-4f76-bb86-fe7b8eb4af76');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Selective import Scikit Learn\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Generate Training and Validation Sets\n",
        "x_train, x_test, y_train, y_test = train_test_split(x,y, test_size=0.3) #0.3 data as data test\n",
        "\n",
        "#converting to float 32bit\n",
        "x_train = np.array(x_train).astype(np.float32)\n",
        "x_test  = np.array(x_test).astype(np.float32)\n",
        "y_train = np.array(y_train).astype(np.float32)\n",
        "y_test  = np.array(y_test).astype(np.float32)\n",
        "\n",
        "#print data split for validation\n",
        "print(x_train.shape, y_train.shape)\n",
        "print(x_test.shape, y_test.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VzA6kxG69GV8",
        "outputId": "ad3b8edd-eab1-48a9-ce0f-936e9f8f6f19"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(105, 4) (105, 3)\n",
            "(45, 4) (45, 3)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Importing our model\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "\n",
        "#model initialization\n",
        "Model = MLPClassifier(hidden_layer_sizes=(10,5), max_iter=2000, alpha=0.01, #try change hidden layer\n",
        "                     solver='sgd', verbose=1,  random_state=121) #try verbode=0 to train with out logging\n",
        "#train our model\n",
        "h=Model.fit(x_train,y_train)\n",
        "#use our model to predict\n",
        "y_pred=Model.predict(x_test)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XuM6-Iiu9JNt",
        "outputId": "e03fab5b-24ac-4a88-a9ca-019bdab7aee9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iteration 1, loss = 2.62491626\n",
            "Iteration 2, loss = 2.58142197\n",
            "Iteration 3, loss = 2.52428789\n",
            "Iteration 4, loss = 2.45861913\n",
            "Iteration 5, loss = 2.39114182\n",
            "Iteration 6, loss = 2.32487890\n",
            "Iteration 7, loss = 2.26159368\n",
            "Iteration 8, loss = 2.20198390\n",
            "Iteration 9, loss = 2.14733603\n",
            "Iteration 10, loss = 2.09827475\n",
            "Iteration 11, loss = 2.05494160\n",
            "Iteration 12, loss = 2.01835459\n",
            "Iteration 13, loss = 1.98811314\n",
            "Iteration 14, loss = 1.96337233\n",
            "Iteration 15, loss = 1.94438117\n",
            "Iteration 16, loss = 1.93003296\n",
            "Iteration 17, loss = 1.91984636\n",
            "Iteration 18, loss = 1.91244803\n",
            "Iteration 19, loss = 1.90694084\n",
            "Iteration 20, loss = 1.90286654\n",
            "Iteration 21, loss = 1.89955253\n",
            "Iteration 22, loss = 1.89644269\n",
            "Iteration 23, loss = 1.89319762\n",
            "Iteration 24, loss = 1.88963141\n",
            "Iteration 25, loss = 1.88564826\n",
            "Iteration 26, loss = 1.88128363\n",
            "Iteration 27, loss = 1.87662507\n",
            "Iteration 28, loss = 1.87177853\n",
            "Iteration 29, loss = 1.86673788\n",
            "Iteration 30, loss = 1.86146728\n",
            "Iteration 31, loss = 1.85607721\n",
            "Iteration 32, loss = 1.85068385\n",
            "Iteration 33, loss = 1.84526195\n",
            "Iteration 34, loss = 1.83986576\n",
            "Iteration 35, loss = 1.83453924\n",
            "Iteration 36, loss = 1.82928135\n",
            "Iteration 37, loss = 1.82411347\n",
            "Iteration 38, loss = 1.81904377\n",
            "Iteration 39, loss = 1.81408233\n",
            "Iteration 40, loss = 1.80923240\n",
            "Iteration 41, loss = 1.80449607\n",
            "Iteration 42, loss = 1.79994478\n",
            "Iteration 43, loss = 1.79557429\n",
            "Iteration 44, loss = 1.79142161\n",
            "Iteration 45, loss = 1.78757397\n",
            "Iteration 46, loss = 1.78391030\n",
            "Iteration 47, loss = 1.78037530\n",
            "Iteration 48, loss = 1.77692458\n",
            "Iteration 49, loss = 1.77354624\n",
            "Iteration 50, loss = 1.77030845\n",
            "Iteration 51, loss = 1.76720125\n",
            "Iteration 52, loss = 1.76415277\n",
            "Iteration 53, loss = 1.76109987\n",
            "Iteration 54, loss = 1.75806296\n",
            "Iteration 55, loss = 1.75501236\n",
            "Iteration 56, loss = 1.75199997\n",
            "Iteration 57, loss = 1.74901220\n",
            "Iteration 58, loss = 1.74605016\n",
            "Iteration 59, loss = 1.74312820\n",
            "Iteration 60, loss = 1.74025063\n",
            "Iteration 61, loss = 1.73737902\n",
            "Iteration 62, loss = 1.73450857\n",
            "Iteration 63, loss = 1.73165349\n",
            "Iteration 64, loss = 1.72880276\n",
            "Iteration 65, loss = 1.72597894\n",
            "Iteration 66, loss = 1.72320797\n",
            "Iteration 67, loss = 1.72043145\n",
            "Iteration 68, loss = 1.71766787\n",
            "Iteration 69, loss = 1.71492830\n",
            "Iteration 70, loss = 1.71218493\n",
            "Iteration 71, loss = 1.70943682\n",
            "Iteration 72, loss = 1.70668269\n",
            "Iteration 73, loss = 1.70392100\n",
            "Iteration 74, loss = 1.70117132\n",
            "Iteration 75, loss = 1.69841587\n",
            "Iteration 76, loss = 1.69566013\n",
            "Iteration 77, loss = 1.69290543\n",
            "Iteration 78, loss = 1.69014406\n",
            "Iteration 79, loss = 1.68737046\n",
            "Iteration 80, loss = 1.68457577\n",
            "Iteration 81, loss = 1.68175710\n",
            "Iteration 82, loss = 1.67891957\n",
            "Iteration 83, loss = 1.67606144\n",
            "Iteration 84, loss = 1.67317926\n",
            "Iteration 85, loss = 1.67027198\n",
            "Iteration 86, loss = 1.66733833\n",
            "Iteration 87, loss = 1.66437686\n",
            "Iteration 88, loss = 1.66138602\n",
            "Iteration 89, loss = 1.65837415\n",
            "Iteration 90, loss = 1.65534153\n",
            "Iteration 91, loss = 1.65228009\n",
            "Iteration 92, loss = 1.64919008\n",
            "Iteration 93, loss = 1.64607141\n",
            "Iteration 94, loss = 1.64292425\n",
            "Iteration 95, loss = 1.63974789\n",
            "Iteration 96, loss = 1.63654217\n",
            "Iteration 97, loss = 1.63330930\n",
            "Iteration 98, loss = 1.63005847\n",
            "Iteration 99, loss = 1.62677573\n",
            "Iteration 100, loss = 1.62346847\n",
            "Iteration 101, loss = 1.62012973\n",
            "Iteration 102, loss = 1.61675803\n",
            "Iteration 103, loss = 1.61335692\n",
            "Iteration 104, loss = 1.60992126\n",
            "Iteration 105, loss = 1.60647138\n",
            "Iteration 106, loss = 1.60299798\n",
            "Iteration 107, loss = 1.59950028\n",
            "Iteration 108, loss = 1.59598486\n",
            "Iteration 109, loss = 1.59243360\n",
            "Iteration 110, loss = 1.58884882\n",
            "Iteration 111, loss = 1.58523253\n",
            "Iteration 112, loss = 1.58159302\n",
            "Iteration 113, loss = 1.57792853\n",
            "Iteration 114, loss = 1.57423584\n",
            "Iteration 115, loss = 1.57051489\n",
            "Iteration 116, loss = 1.56676508\n",
            "Iteration 117, loss = 1.56298588\n",
            "Iteration 118, loss = 1.55918032\n",
            "Iteration 119, loss = 1.55534322\n",
            "Iteration 120, loss = 1.55147333\n",
            "Iteration 121, loss = 1.54759834\n",
            "Iteration 122, loss = 1.54372438\n",
            "Iteration 123, loss = 1.53978879\n",
            "Iteration 124, loss = 1.53584317\n",
            "Iteration 125, loss = 1.53188238\n",
            "Iteration 126, loss = 1.52790697\n",
            "Iteration 127, loss = 1.52390890\n",
            "Iteration 128, loss = 1.51989693\n",
            "Iteration 129, loss = 1.51586517\n",
            "Iteration 130, loss = 1.51180791\n",
            "Iteration 131, loss = 1.50773559\n",
            "Iteration 132, loss = 1.50366094\n",
            "Iteration 133, loss = 1.49958186\n",
            "Iteration 134, loss = 1.49551632\n",
            "Iteration 135, loss = 1.49145918\n",
            "Iteration 136, loss = 1.48738888\n",
            "Iteration 137, loss = 1.48330394\n",
            "Iteration 138, loss = 1.47920823\n",
            "Iteration 139, loss = 1.47512487\n",
            "Iteration 140, loss = 1.47105962\n",
            "Iteration 141, loss = 1.46698957\n",
            "Iteration 142, loss = 1.46291091\n",
            "Iteration 143, loss = 1.45884131\n",
            "Iteration 144, loss = 1.45477723\n",
            "Iteration 145, loss = 1.45070298\n",
            "Iteration 146, loss = 1.44662227\n",
            "Iteration 147, loss = 1.44254184\n",
            "Iteration 148, loss = 1.43844949\n",
            "Iteration 149, loss = 1.43435928\n",
            "Iteration 150, loss = 1.43025693\n",
            "Iteration 151, loss = 1.42614292\n",
            "Iteration 152, loss = 1.42201727\n",
            "Iteration 153, loss = 1.41788038\n",
            "Iteration 154, loss = 1.41373500\n",
            "Iteration 155, loss = 1.40958190\n",
            "Iteration 156, loss = 1.40541885\n",
            "Iteration 157, loss = 1.40125216\n",
            "Iteration 158, loss = 1.39707442\n",
            "Iteration 159, loss = 1.39288724\n",
            "Iteration 160, loss = 1.38870007\n",
            "Iteration 161, loss = 1.38450775\n",
            "Iteration 162, loss = 1.38030728\n",
            "Iteration 163, loss = 1.37610049\n",
            "Iteration 164, loss = 1.37189070\n",
            "Iteration 165, loss = 1.36768526\n",
            "Iteration 166, loss = 1.36346876\n",
            "Iteration 167, loss = 1.35925044\n",
            "Iteration 168, loss = 1.35502860\n",
            "Iteration 169, loss = 1.35080621\n",
            "Iteration 170, loss = 1.34657772\n",
            "Iteration 171, loss = 1.34234598\n",
            "Iteration 172, loss = 1.33811631\n",
            "Iteration 173, loss = 1.33388092\n",
            "Iteration 174, loss = 1.32964558\n",
            "Iteration 175, loss = 1.32541300\n",
            "Iteration 176, loss = 1.32118558\n",
            "Iteration 177, loss = 1.31694922\n",
            "Iteration 178, loss = 1.31271495\n",
            "Iteration 179, loss = 1.30848775\n",
            "Iteration 180, loss = 1.30426379\n",
            "Iteration 181, loss = 1.30004274\n",
            "Iteration 182, loss = 1.29582816\n",
            "Iteration 183, loss = 1.29161444\n",
            "Iteration 184, loss = 1.28740689\n",
            "Iteration 185, loss = 1.28320463\n",
            "Iteration 186, loss = 1.27900815\n",
            "Iteration 187, loss = 1.27481828\n",
            "Iteration 188, loss = 1.27063579\n",
            "Iteration 189, loss = 1.26646087\n",
            "Iteration 190, loss = 1.26229438\n",
            "Iteration 191, loss = 1.25813672\n",
            "Iteration 192, loss = 1.25398826\n",
            "Iteration 193, loss = 1.24984958\n",
            "Iteration 194, loss = 1.24572404\n",
            "Iteration 195, loss = 1.24160738\n",
            "Iteration 196, loss = 1.23750264\n",
            "Iteration 197, loss = 1.23341718\n",
            "Iteration 198, loss = 1.22934018\n",
            "Iteration 199, loss = 1.22527990\n",
            "Iteration 200, loss = 1.22123527\n",
            "Iteration 201, loss = 1.21720506\n",
            "Iteration 202, loss = 1.21319114\n",
            "Iteration 203, loss = 1.20919729\n",
            "Iteration 204, loss = 1.20521723\n",
            "Iteration 205, loss = 1.20125641\n",
            "Iteration 206, loss = 1.19731397\n",
            "Iteration 207, loss = 1.19339017\n",
            "Iteration 208, loss = 1.18948549\n",
            "Iteration 209, loss = 1.18560001\n",
            "Iteration 210, loss = 1.18173408\n",
            "Iteration 211, loss = 1.17788876\n",
            "Iteration 212, loss = 1.17406679\n",
            "Iteration 213, loss = 1.17026898\n",
            "Iteration 214, loss = 1.16649543\n",
            "Iteration 215, loss = 1.16275346\n",
            "Iteration 216, loss = 1.15902004\n",
            "Iteration 217, loss = 1.15531717\n",
            "Iteration 218, loss = 1.15164142\n",
            "Iteration 219, loss = 1.14799059\n",
            "Iteration 220, loss = 1.14435749\n",
            "Iteration 221, loss = 1.14076307\n",
            "Iteration 222, loss = 1.13717962\n",
            "Iteration 223, loss = 1.13362837\n",
            "Iteration 224, loss = 1.13010091\n",
            "Iteration 225, loss = 1.12660163\n",
            "Iteration 226, loss = 1.12312877\n",
            "Iteration 227, loss = 1.11968080\n",
            "Iteration 228, loss = 1.11625992\n",
            "Iteration 229, loss = 1.11286596\n",
            "Iteration 230, loss = 1.10949886\n",
            "Iteration 231, loss = 1.10615853\n",
            "Iteration 232, loss = 1.10284478\n",
            "Iteration 233, loss = 1.09955721\n",
            "Iteration 234, loss = 1.09629653\n",
            "Iteration 235, loss = 1.09306076\n",
            "Iteration 236, loss = 1.08985756\n",
            "Iteration 237, loss = 1.08668089\n",
            "Iteration 238, loss = 1.08353153\n",
            "Iteration 239, loss = 1.08041084\n",
            "Iteration 240, loss = 1.07732107\n",
            "Iteration 241, loss = 1.07425768\n",
            "Iteration 242, loss = 1.07122238\n",
            "Iteration 243, loss = 1.06821509\n",
            "Iteration 244, loss = 1.06523603\n",
            "Iteration 245, loss = 1.06228494\n",
            "Iteration 246, loss = 1.05936129\n",
            "Iteration 247, loss = 1.05646443\n",
            "Iteration 248, loss = 1.05359464\n",
            "Iteration 249, loss = 1.05075023\n",
            "Iteration 250, loss = 1.04793217\n",
            "Iteration 251, loss = 1.04513876\n",
            "Iteration 252, loss = 1.04237138\n",
            "Iteration 253, loss = 1.03962821\n",
            "Iteration 254, loss = 1.03691095\n",
            "Iteration 255, loss = 1.03421789\n",
            "Iteration 256, loss = 1.03154999\n",
            "Iteration 257, loss = 1.02890617\n",
            "Iteration 258, loss = 1.02628750\n",
            "Iteration 259, loss = 1.02369217\n",
            "Iteration 260, loss = 1.02112191\n",
            "Iteration 261, loss = 1.01857448\n",
            "Iteration 262, loss = 1.01605149\n",
            "Iteration 263, loss = 1.01355117\n",
            "Iteration 264, loss = 1.01107458\n",
            "Iteration 265, loss = 1.00862044\n",
            "Iteration 266, loss = 1.00618921\n",
            "Iteration 267, loss = 1.00378029\n",
            "Iteration 268, loss = 1.00139347\n",
            "Iteration 269, loss = 0.99902910\n",
            "Iteration 270, loss = 0.99668573\n",
            "Iteration 271, loss = 0.99436501\n",
            "Iteration 272, loss = 0.99206463\n",
            "Iteration 273, loss = 0.98978610\n",
            "Iteration 274, loss = 0.98752769\n",
            "Iteration 275, loss = 0.98530699\n",
            "Iteration 276, loss = 0.98310637\n",
            "Iteration 277, loss = 0.98092651\n",
            "Iteration 278, loss = 0.97876836\n",
            "Iteration 279, loss = 0.97663227\n",
            "Iteration 280, loss = 0.97451795\n",
            "Iteration 281, loss = 0.97242579\n",
            "Iteration 282, loss = 0.97035513\n",
            "Iteration 283, loss = 0.96830581\n",
            "Iteration 284, loss = 0.96627617\n",
            "Iteration 285, loss = 0.96426738\n",
            "Iteration 286, loss = 0.96228172\n",
            "Iteration 287, loss = 0.96032356\n",
            "Iteration 288, loss = 0.95838492\n",
            "Iteration 289, loss = 0.95646682\n",
            "Iteration 290, loss = 0.95456961\n",
            "Iteration 291, loss = 0.95269225\n",
            "Iteration 292, loss = 0.95083754\n",
            "Iteration 293, loss = 0.94900392\n",
            "Iteration 294, loss = 0.94719180\n",
            "Iteration 295, loss = 0.94539987\n",
            "Iteration 296, loss = 0.94362708\n",
            "Iteration 297, loss = 0.94187319\n",
            "Iteration 298, loss = 0.94013698\n",
            "Iteration 299, loss = 0.93841805\n",
            "Iteration 300, loss = 0.93671445\n",
            "Iteration 301, loss = 0.93502636\n",
            "Iteration 302, loss = 0.93335416\n",
            "Iteration 303, loss = 0.93169565\n",
            "Iteration 304, loss = 0.93005244\n",
            "Iteration 305, loss = 0.92842466\n",
            "Iteration 306, loss = 0.92681150\n",
            "Iteration 307, loss = 0.92521301\n",
            "Iteration 308, loss = 0.92362977\n",
            "Iteration 309, loss = 0.92207059\n",
            "Iteration 310, loss = 0.92052512\n",
            "Iteration 311, loss = 0.91899214\n",
            "Iteration 312, loss = 0.91747221\n",
            "Iteration 313, loss = 0.91596552\n",
            "Iteration 314, loss = 0.91447346\n",
            "Iteration 315, loss = 0.91299494\n",
            "Iteration 316, loss = 0.91153078\n",
            "Iteration 317, loss = 0.91007919\n",
            "Iteration 318, loss = 0.90864167\n",
            "Iteration 319, loss = 0.90721577\n",
            "Iteration 320, loss = 0.90580330\n",
            "Iteration 321, loss = 0.90440200\n",
            "Iteration 322, loss = 0.90301218\n",
            "Iteration 323, loss = 0.90163494\n",
            "Iteration 324, loss = 0.90026750\n",
            "Iteration 325, loss = 0.89891215\n",
            "Iteration 326, loss = 0.89756730\n",
            "Iteration 327, loss = 0.89623301\n",
            "Iteration 328, loss = 0.89491040\n",
            "Iteration 329, loss = 0.89359736\n",
            "Iteration 330, loss = 0.89229598\n",
            "Iteration 331, loss = 0.89100390\n",
            "Iteration 332, loss = 0.88972158\n",
            "Iteration 333, loss = 0.88845037\n",
            "Iteration 334, loss = 0.88718763\n",
            "Iteration 335, loss = 0.88593526\n",
            "Iteration 336, loss = 0.88469185\n",
            "Iteration 337, loss = 0.88346174\n",
            "Iteration 338, loss = 0.88223942\n",
            "Iteration 339, loss = 0.88102519\n",
            "Iteration 340, loss = 0.87982084\n",
            "Iteration 341, loss = 0.87862910\n",
            "Iteration 342, loss = 0.87744347\n",
            "Iteration 343, loss = 0.87626921\n",
            "Iteration 344, loss = 0.87510323\n",
            "Iteration 345, loss = 0.87394622\n",
            "Iteration 346, loss = 0.87279873\n",
            "Iteration 347, loss = 0.87165891\n",
            "Iteration 348, loss = 0.87052851\n",
            "Iteration 349, loss = 0.86940553\n",
            "Iteration 350, loss = 0.86829016\n",
            "Iteration 351, loss = 0.86718365\n",
            "Iteration 352, loss = 0.86608299\n",
            "Iteration 353, loss = 0.86499115\n",
            "Iteration 354, loss = 0.86390580\n",
            "Iteration 355, loss = 0.86282741\n",
            "Iteration 356, loss = 0.86175596\n",
            "Iteration 357, loss = 0.86069241\n",
            "Iteration 358, loss = 0.85963498\n",
            "Iteration 359, loss = 0.85858517\n",
            "Iteration 360, loss = 0.85754131\n",
            "Iteration 361, loss = 0.85650471\n",
            "Iteration 362, loss = 0.85547460\n",
            "Iteration 363, loss = 0.85445052\n",
            "Iteration 364, loss = 0.85343352\n",
            "Iteration 365, loss = 0.85242194\n",
            "Iteration 366, loss = 0.85141702\n",
            "Iteration 367, loss = 0.85041812\n",
            "Iteration 368, loss = 0.84942475\n",
            "Iteration 369, loss = 0.84843798\n",
            "Iteration 370, loss = 0.84745635\n",
            "Iteration 371, loss = 0.84648065\n",
            "Iteration 372, loss = 0.84551120\n",
            "Iteration 373, loss = 0.84454661\n",
            "Iteration 374, loss = 0.84358732\n",
            "Iteration 375, loss = 0.84263498\n",
            "Iteration 376, loss = 0.84168607\n",
            "Iteration 377, loss = 0.84074380\n",
            "Iteration 378, loss = 0.83980620\n",
            "Iteration 379, loss = 0.83887357\n",
            "Iteration 380, loss = 0.83794586\n",
            "Iteration 381, loss = 0.83702427\n",
            "Iteration 382, loss = 0.83610605\n",
            "Iteration 383, loss = 0.83519408\n",
            "Iteration 384, loss = 0.83428617\n",
            "Iteration 385, loss = 0.83338309\n",
            "Iteration 386, loss = 0.83248465\n",
            "Iteration 387, loss = 0.83159166\n",
            "Iteration 388, loss = 0.83070250\n",
            "Iteration 389, loss = 0.82981782\n",
            "Iteration 390, loss = 0.82893788\n",
            "Iteration 391, loss = 0.82806234\n",
            "Iteration 392, loss = 0.82719100\n",
            "Iteration 393, loss = 0.82632448\n",
            "Iteration 394, loss = 0.82546298\n",
            "Iteration 395, loss = 0.82460662\n",
            "Iteration 396, loss = 0.82375402\n",
            "Iteration 397, loss = 0.82290577\n",
            "Iteration 398, loss = 0.82206387\n",
            "Iteration 399, loss = 0.82122441\n",
            "Iteration 400, loss = 0.82038929\n",
            "Iteration 401, loss = 0.81955926\n",
            "Iteration 402, loss = 0.81873373\n",
            "Iteration 403, loss = 0.81791114\n",
            "Iteration 404, loss = 0.81709350\n",
            "Iteration 405, loss = 0.81627961\n",
            "Iteration 406, loss = 0.81546940\n",
            "Iteration 407, loss = 0.81466281\n",
            "Iteration 408, loss = 0.81385990\n",
            "Iteration 409, loss = 0.81306163\n",
            "Iteration 410, loss = 0.81226528\n",
            "Iteration 411, loss = 0.81147320\n",
            "Iteration 412, loss = 0.81068515\n",
            "Iteration 413, loss = 0.80990148\n",
            "Iteration 414, loss = 0.80912134\n",
            "Iteration 415, loss = 0.80834474\n",
            "Iteration 416, loss = 0.80757157\n",
            "Iteration 417, loss = 0.80680184\n",
            "Iteration 418, loss = 0.80603543\n",
            "Iteration 419, loss = 0.80527240\n",
            "Iteration 420, loss = 0.80451265\n",
            "Iteration 421, loss = 0.80375659\n",
            "Iteration 422, loss = 0.80300305\n",
            "Iteration 423, loss = 0.80225306\n",
            "Iteration 424, loss = 0.80150623\n",
            "Iteration 425, loss = 0.80076285\n",
            "Iteration 426, loss = 0.80002251\n",
            "Iteration 427, loss = 0.79928499\n",
            "Iteration 428, loss = 0.79855033\n",
            "Iteration 429, loss = 0.79781993\n",
            "Iteration 430, loss = 0.79709147\n",
            "Iteration 431, loss = 0.79636571\n",
            "Iteration 432, loss = 0.79564361\n",
            "Iteration 433, loss = 0.79492393\n",
            "Iteration 434, loss = 0.79420719\n",
            "Iteration 435, loss = 0.79349412\n",
            "Iteration 436, loss = 0.79278229\n",
            "Iteration 437, loss = 0.79207465\n",
            "Iteration 438, loss = 0.79136882\n",
            "Iteration 439, loss = 0.79066589\n",
            "Iteration 440, loss = 0.78996591\n",
            "Iteration 441, loss = 0.78926807\n",
            "Iteration 442, loss = 0.78857323\n",
            "Iteration 443, loss = 0.78788109\n",
            "Iteration 444, loss = 0.78719117\n",
            "Iteration 445, loss = 0.78650387\n",
            "Iteration 446, loss = 0.78582006\n",
            "Iteration 447, loss = 0.78513738\n",
            "Iteration 448, loss = 0.78445755\n",
            "Iteration 449, loss = 0.78378038\n",
            "Iteration 450, loss = 0.78310564\n",
            "Iteration 451, loss = 0.78243333\n",
            "Iteration 452, loss = 0.78176322\n",
            "Iteration 453, loss = 0.78109540\n",
            "Iteration 454, loss = 0.78043037\n",
            "Iteration 455, loss = 0.77976797\n",
            "Iteration 456, loss = 0.77910654\n",
            "Iteration 457, loss = 0.77844786\n",
            "Iteration 458, loss = 0.77779200\n",
            "Iteration 459, loss = 0.77713805\n",
            "Iteration 460, loss = 0.77648618\n",
            "Iteration 461, loss = 0.77583703\n",
            "Iteration 462, loss = 0.77518944\n",
            "Iteration 463, loss = 0.77454481\n",
            "Iteration 464, loss = 0.77390161\n",
            "Iteration 465, loss = 0.77326061\n",
            "Iteration 466, loss = 0.77262201\n",
            "Iteration 467, loss = 0.77198640\n",
            "Iteration 468, loss = 0.77135362\n",
            "Iteration 469, loss = 0.77072222\n",
            "Iteration 470, loss = 0.77009277\n",
            "Iteration 471, loss = 0.76946529\n",
            "Iteration 472, loss = 0.76884015\n",
            "Iteration 473, loss = 0.76821665\n",
            "Iteration 474, loss = 0.76759560\n",
            "Iteration 475, loss = 0.76697609\n",
            "Iteration 476, loss = 0.76635920\n",
            "Iteration 477, loss = 0.76574374\n",
            "Iteration 478, loss = 0.76513044\n",
            "Iteration 479, loss = 0.76451942\n",
            "Iteration 480, loss = 0.76390959\n",
            "Iteration 481, loss = 0.76330252\n",
            "Iteration 482, loss = 0.76269657\n",
            "Iteration 483, loss = 0.76209243\n",
            "Iteration 484, loss = 0.76149051\n",
            "Iteration 485, loss = 0.76089036\n",
            "Iteration 486, loss = 0.76029176\n",
            "Iteration 487, loss = 0.75969540\n",
            "Iteration 488, loss = 0.75910039\n",
            "Iteration 489, loss = 0.75850745\n",
            "Iteration 490, loss = 0.75791648\n",
            "Iteration 491, loss = 0.75732697\n",
            "Iteration 492, loss = 0.75673891\n",
            "Iteration 493, loss = 0.75615275\n",
            "Iteration 494, loss = 0.75556820\n",
            "Iteration 495, loss = 0.75498546\n",
            "Iteration 496, loss = 0.75440432\n",
            "Iteration 497, loss = 0.75382469\n",
            "Iteration 498, loss = 0.75324709\n",
            "Iteration 499, loss = 0.75267073\n",
            "Iteration 500, loss = 0.75209604\n",
            "Iteration 501, loss = 0.75152330\n",
            "Iteration 502, loss = 0.75095172\n",
            "Iteration 503, loss = 0.75038177\n",
            "Iteration 504, loss = 0.74981338\n",
            "Iteration 505, loss = 0.74924693\n",
            "Iteration 506, loss = 0.74868171\n",
            "Iteration 507, loss = 0.74811797\n",
            "Iteration 508, loss = 0.74755566\n",
            "Iteration 509, loss = 0.74699498\n",
            "Iteration 510, loss = 0.74643589\n",
            "Iteration 511, loss = 0.74587811\n",
            "Iteration 512, loss = 0.74532167\n",
            "Iteration 513, loss = 0.74476680\n",
            "Iteration 514, loss = 0.74421368\n",
            "Iteration 515, loss = 0.74366152\n",
            "Iteration 516, loss = 0.74311095\n",
            "Iteration 517, loss = 0.74256174\n",
            "Iteration 518, loss = 0.74201394\n",
            "Iteration 519, loss = 0.74146766\n",
            "Iteration 520, loss = 0.74092262\n",
            "Iteration 521, loss = 0.74037898\n",
            "Iteration 522, loss = 0.73983660\n",
            "Iteration 523, loss = 0.73929558\n",
            "Iteration 524, loss = 0.73875587\n",
            "Iteration 525, loss = 0.73821744\n",
            "Iteration 526, loss = 0.73768028\n",
            "Iteration 527, loss = 0.73714459\n",
            "Iteration 528, loss = 0.73661013\n",
            "Iteration 529, loss = 0.73607692\n",
            "Iteration 530, loss = 0.73554492\n",
            "Iteration 531, loss = 0.73501426\n",
            "Iteration 532, loss = 0.73448472\n",
            "Iteration 533, loss = 0.73395644\n",
            "Iteration 534, loss = 0.73342939\n",
            "Iteration 535, loss = 0.73290362\n",
            "Iteration 536, loss = 0.73237928\n",
            "Iteration 537, loss = 0.73185566\n",
            "Iteration 538, loss = 0.73133337\n",
            "Iteration 539, loss = 0.73081225\n",
            "Iteration 540, loss = 0.73029224\n",
            "Iteration 541, loss = 0.72977342\n",
            "Iteration 542, loss = 0.72925576\n",
            "Iteration 543, loss = 0.72873921\n",
            "Iteration 544, loss = 0.72822368\n",
            "Iteration 545, loss = 0.72770936\n",
            "Iteration 546, loss = 0.72719608\n",
            "Iteration 547, loss = 0.72668396\n",
            "Iteration 548, loss = 0.72617290\n",
            "Iteration 549, loss = 0.72566282\n",
            "Iteration 550, loss = 0.72515387\n",
            "Iteration 551, loss = 0.72464600\n",
            "Iteration 552, loss = 0.72413909\n",
            "Iteration 553, loss = 0.72363325\n",
            "Iteration 554, loss = 0.72312836\n",
            "Iteration 555, loss = 0.72262460\n",
            "Iteration 556, loss = 0.72212179\n",
            "Iteration 557, loss = 0.72161996\n",
            "Iteration 558, loss = 0.72111910\n",
            "Iteration 559, loss = 0.72061922\n",
            "Iteration 560, loss = 0.72012029\n",
            "Iteration 561, loss = 0.71962233\n",
            "Iteration 562, loss = 0.71912536\n",
            "Iteration 563, loss = 0.71862924\n",
            "Iteration 564, loss = 0.71813412\n",
            "Iteration 565, loss = 0.71763985\n",
            "Iteration 566, loss = 0.71714650\n",
            "Iteration 567, loss = 0.71665411\n",
            "Iteration 568, loss = 0.71616253\n",
            "Iteration 569, loss = 0.71567186\n",
            "Iteration 570, loss = 0.71518204\n",
            "Iteration 571, loss = 0.71469319\n",
            "Iteration 572, loss = 0.71420510\n",
            "Iteration 573, loss = 0.71371787\n",
            "Iteration 574, loss = 0.71323143\n",
            "Iteration 575, loss = 0.71274592\n",
            "Iteration 576, loss = 0.71226114\n",
            "Iteration 577, loss = 0.71177719\n",
            "Iteration 578, loss = 0.71129406\n",
            "Iteration 579, loss = 0.71081176\n",
            "Iteration 580, loss = 0.71033019\n",
            "Iteration 581, loss = 0.70984958\n",
            "Iteration 582, loss = 0.70936985\n",
            "Iteration 583, loss = 0.70889100\n",
            "Iteration 584, loss = 0.70841298\n",
            "Iteration 585, loss = 0.70793567\n",
            "Iteration 586, loss = 0.70745948\n",
            "Iteration 587, loss = 0.70698442\n",
            "Iteration 588, loss = 0.70651055\n",
            "Iteration 589, loss = 0.70603717\n",
            "Iteration 590, loss = 0.70556437\n",
            "Iteration 591, loss = 0.70509254\n",
            "Iteration 592, loss = 0.70462157\n",
            "Iteration 593, loss = 0.70415107\n",
            "Iteration 594, loss = 0.70368125\n",
            "Iteration 595, loss = 0.70321299\n",
            "Iteration 596, loss = 0.70274432\n",
            "Iteration 597, loss = 0.70227671\n",
            "Iteration 598, loss = 0.70181038\n",
            "Iteration 599, loss = 0.70134433\n",
            "Iteration 600, loss = 0.70087887\n",
            "Iteration 601, loss = 0.70041393\n",
            "Iteration 602, loss = 0.69995027\n",
            "Iteration 603, loss = 0.69948653\n",
            "Iteration 604, loss = 0.69902394\n",
            "Iteration 605, loss = 0.69856205\n",
            "Iteration 606, loss = 0.69810061\n",
            "Iteration 607, loss = 0.69763969\n",
            "Iteration 608, loss = 0.69717993\n",
            "Iteration 609, loss = 0.69672037\n",
            "Iteration 610, loss = 0.69626145\n",
            "Iteration 611, loss = 0.69580353\n",
            "Iteration 612, loss = 0.69534578\n",
            "Iteration 613, loss = 0.69488925\n",
            "Iteration 614, loss = 0.69443244\n",
            "Iteration 615, loss = 0.69397695\n",
            "Iteration 616, loss = 0.69352172\n",
            "Iteration 617, loss = 0.69306691\n",
            "Iteration 618, loss = 0.69261355\n",
            "Iteration 619, loss = 0.69215950\n",
            "Iteration 620, loss = 0.69170735\n",
            "Iteration 621, loss = 0.69125489\n",
            "Iteration 622, loss = 0.69080279\n",
            "Iteration 623, loss = 0.69035207\n",
            "Iteration 624, loss = 0.68990100\n",
            "Iteration 625, loss = 0.68945115\n",
            "Iteration 626, loss = 0.68900129\n",
            "Iteration 627, loss = 0.68855255\n",
            "Iteration 628, loss = 0.68810360\n",
            "Iteration 629, loss = 0.68765609\n",
            "Iteration 630, loss = 0.68720828\n",
            "Iteration 631, loss = 0.68676128\n",
            "Iteration 632, loss = 0.68631444\n",
            "Iteration 633, loss = 0.68586893\n",
            "Iteration 634, loss = 0.68542303\n",
            "Iteration 635, loss = 0.68497828\n",
            "Iteration 636, loss = 0.68453324\n",
            "Iteration 637, loss = 0.68408954\n",
            "Iteration 638, loss = 0.68364564\n",
            "Iteration 639, loss = 0.68320239\n",
            "Iteration 640, loss = 0.68275947\n",
            "Iteration 641, loss = 0.68231725\n",
            "Iteration 642, loss = 0.68187537\n",
            "Iteration 643, loss = 0.68143377\n",
            "Iteration 644, loss = 0.68099335\n",
            "Iteration 645, loss = 0.68055260\n",
            "Iteration 646, loss = 0.68011266\n",
            "Iteration 647, loss = 0.67967290\n",
            "Iteration 648, loss = 0.67923370\n",
            "Iteration 649, loss = 0.67879506\n",
            "Iteration 650, loss = 0.67835636\n",
            "Iteration 651, loss = 0.67791873\n",
            "Iteration 652, loss = 0.67748082\n",
            "Iteration 653, loss = 0.67704358\n",
            "Iteration 654, loss = 0.67660693\n",
            "Iteration 655, loss = 0.67617027\n",
            "Iteration 656, loss = 0.67573403\n",
            "Iteration 657, loss = 0.67529854\n",
            "Iteration 658, loss = 0.67486297\n",
            "Iteration 659, loss = 0.67442783\n",
            "Iteration 660, loss = 0.67399332\n",
            "Iteration 661, loss = 0.67355879\n",
            "Iteration 662, loss = 0.67312487\n",
            "Iteration 663, loss = 0.67269140\n",
            "Iteration 664, loss = 0.67225833\n",
            "Iteration 665, loss = 0.67182646\n",
            "Iteration 666, loss = 0.67139385\n",
            "Iteration 667, loss = 0.67096179\n",
            "Iteration 668, loss = 0.67053068\n",
            "Iteration 669, loss = 0.67009926\n",
            "Iteration 670, loss = 0.66966875\n",
            "Iteration 671, loss = 0.66923869\n",
            "Iteration 672, loss = 0.66880875\n",
            "Iteration 673, loss = 0.66837841\n",
            "Iteration 674, loss = 0.66794946\n",
            "Iteration 675, loss = 0.66752028\n",
            "Iteration 676, loss = 0.66709112\n",
            "Iteration 677, loss = 0.66666246\n",
            "Iteration 678, loss = 0.66623404\n",
            "Iteration 679, loss = 0.66580660\n",
            "Iteration 680, loss = 0.66537803\n",
            "Iteration 681, loss = 0.66495121\n",
            "Iteration 682, loss = 0.66452411\n",
            "Iteration 683, loss = 0.66409708\n",
            "Iteration 684, loss = 0.66367015\n",
            "Iteration 685, loss = 0.66324329\n",
            "Iteration 686, loss = 0.66281734\n",
            "Iteration 687, loss = 0.66239078\n",
            "Iteration 688, loss = 0.66196520\n",
            "Iteration 689, loss = 0.66153936\n",
            "Iteration 690, loss = 0.66111384\n",
            "Iteration 691, loss = 0.66068902\n",
            "Iteration 692, loss = 0.66026382\n",
            "Iteration 693, loss = 0.65983954\n",
            "Iteration 694, loss = 0.65941462\n",
            "Iteration 695, loss = 0.65899019\n",
            "Iteration 696, loss = 0.65856583\n",
            "Iteration 697, loss = 0.65814213\n",
            "Iteration 698, loss = 0.65771823\n",
            "Iteration 699, loss = 0.65729467\n",
            "Iteration 700, loss = 0.65687119\n",
            "Iteration 701, loss = 0.65644780\n",
            "Iteration 702, loss = 0.65602519\n",
            "Iteration 703, loss = 0.65560198\n",
            "Iteration 704, loss = 0.65517906\n",
            "Iteration 705, loss = 0.65475627\n",
            "Iteration 706, loss = 0.65433405\n",
            "Iteration 707, loss = 0.65391165\n",
            "Iteration 708, loss = 0.65348936\n",
            "Iteration 709, loss = 0.65306750\n",
            "Iteration 710, loss = 0.65264539\n",
            "Iteration 711, loss = 0.65222363\n",
            "Iteration 712, loss = 0.65180219\n",
            "Iteration 713, loss = 0.65138055\n",
            "Iteration 714, loss = 0.65095898\n",
            "Iteration 715, loss = 0.65053768\n",
            "Iteration 716, loss = 0.65011658\n",
            "Iteration 717, loss = 0.64969542\n",
            "Iteration 718, loss = 0.64927425\n",
            "Iteration 719, loss = 0.64885324\n",
            "Iteration 720, loss = 0.64843260\n",
            "Iteration 721, loss = 0.64801189\n",
            "Iteration 722, loss = 0.64759121\n",
            "Iteration 723, loss = 0.64717052\n",
            "Iteration 724, loss = 0.64675031\n",
            "Iteration 725, loss = 0.64633013\n",
            "Iteration 726, loss = 0.64590988\n",
            "Iteration 727, loss = 0.64548969\n",
            "Iteration 728, loss = 0.64506996\n",
            "Iteration 729, loss = 0.64465048\n",
            "Iteration 730, loss = 0.64423067\n",
            "Iteration 731, loss = 0.64381080\n",
            "Iteration 732, loss = 0.64339075\n",
            "Iteration 733, loss = 0.64297069\n",
            "Iteration 734, loss = 0.64255053\n",
            "Iteration 735, loss = 0.64213043\n",
            "Iteration 736, loss = 0.64171061\n",
            "Iteration 737, loss = 0.64129220\n",
            "Iteration 738, loss = 0.64087294\n",
            "Iteration 739, loss = 0.64045503\n",
            "Iteration 740, loss = 0.64003814\n",
            "Iteration 741, loss = 0.63962113\n",
            "Iteration 742, loss = 0.63920406\n",
            "Iteration 743, loss = 0.63878692\n",
            "Iteration 744, loss = 0.63836975\n",
            "Iteration 745, loss = 0.63795263\n",
            "Iteration 746, loss = 0.63753637\n",
            "Iteration 747, loss = 0.63711943\n",
            "Iteration 748, loss = 0.63670273\n",
            "Iteration 749, loss = 0.63628581\n",
            "Iteration 750, loss = 0.63587000\n",
            "Iteration 751, loss = 0.63545326\n",
            "Iteration 752, loss = 0.63503670\n",
            "Iteration 753, loss = 0.63461998\n",
            "Iteration 754, loss = 0.63420298\n",
            "Iteration 755, loss = 0.63378787\n",
            "Iteration 756, loss = 0.63337017\n",
            "Iteration 757, loss = 0.63295336\n",
            "Iteration 758, loss = 0.63253670\n",
            "Iteration 759, loss = 0.63212072\n",
            "Iteration 760, loss = 0.63170391\n",
            "Iteration 761, loss = 0.63128691\n",
            "Iteration 762, loss = 0.63086962\n",
            "Iteration 763, loss = 0.63045257\n",
            "Iteration 764, loss = 0.63003594\n",
            "Iteration 765, loss = 0.62961870\n",
            "Iteration 766, loss = 0.62920111\n",
            "Iteration 767, loss = 0.62878450\n",
            "Iteration 768, loss = 0.62836681\n",
            "Iteration 769, loss = 0.62794910\n",
            "Iteration 770, loss = 0.62753123\n",
            "Iteration 771, loss = 0.62711495\n",
            "Iteration 772, loss = 0.62669603\n",
            "Iteration 773, loss = 0.62627795\n",
            "Iteration 774, loss = 0.62586050\n",
            "Iteration 775, loss = 0.62544243\n",
            "Iteration 776, loss = 0.62502405\n",
            "Iteration 777, loss = 0.62460531\n",
            "Iteration 778, loss = 0.62418726\n",
            "Iteration 779, loss = 0.62376848\n",
            "Iteration 780, loss = 0.62334938\n",
            "Iteration 781, loss = 0.62293046\n",
            "Iteration 782, loss = 0.62251199\n",
            "Iteration 783, loss = 0.62209260\n",
            "Iteration 784, loss = 0.62167275\n",
            "Iteration 785, loss = 0.62125406\n",
            "Iteration 786, loss = 0.62083380\n",
            "Iteration 787, loss = 0.62041349\n",
            "Iteration 788, loss = 0.61999466\n",
            "Iteration 789, loss = 0.61957383\n",
            "Iteration 790, loss = 0.61915308\n",
            "Iteration 791, loss = 0.61873323\n",
            "Iteration 792, loss = 0.61831249\n",
            "Iteration 793, loss = 0.61789128\n",
            "Iteration 794, loss = 0.61747023\n",
            "Iteration 795, loss = 0.61704954\n",
            "Iteration 796, loss = 0.61662772\n",
            "Iteration 797, loss = 0.61620570\n",
            "Iteration 798, loss = 0.61578478\n",
            "Iteration 799, loss = 0.61536245\n",
            "Iteration 800, loss = 0.61493948\n",
            "Iteration 801, loss = 0.61451816\n",
            "Iteration 802, loss = 0.61409509\n",
            "Iteration 803, loss = 0.61367186\n",
            "Iteration 804, loss = 0.61324952\n",
            "Iteration 805, loss = 0.61282586\n",
            "Iteration 806, loss = 0.61240249\n",
            "Iteration 807, loss = 0.61197890\n",
            "Iteration 808, loss = 0.61155437\n",
            "Iteration 809, loss = 0.61113468\n",
            "Iteration 810, loss = 0.61070825\n",
            "Iteration 811, loss = 0.61028487\n",
            "Iteration 812, loss = 0.60986117\n",
            "Iteration 813, loss = 0.60943830\n",
            "Iteration 814, loss = 0.60901441\n",
            "Iteration 815, loss = 0.60859009\n",
            "Iteration 816, loss = 0.60816712\n",
            "Iteration 817, loss = 0.60774183\n",
            "Iteration 818, loss = 0.60731810\n",
            "Iteration 819, loss = 0.60689318\n",
            "Iteration 820, loss = 0.60646823\n",
            "Iteration 821, loss = 0.60604333\n",
            "Iteration 822, loss = 0.60561848\n",
            "Iteration 823, loss = 0.60519288\n",
            "Iteration 824, loss = 0.60476697\n",
            "Iteration 825, loss = 0.60434179\n",
            "Iteration 826, loss = 0.60391556\n",
            "Iteration 827, loss = 0.60348893\n",
            "Iteration 828, loss = 0.60306363\n",
            "Iteration 829, loss = 0.60263585\n",
            "Iteration 830, loss = 0.60220952\n",
            "Iteration 831, loss = 0.60178214\n",
            "Iteration 832, loss = 0.60135463\n",
            "Iteration 833, loss = 0.60092677\n",
            "Iteration 834, loss = 0.60049969\n",
            "Iteration 835, loss = 0.60007095\n",
            "Iteration 836, loss = 0.59964237\n",
            "Iteration 837, loss = 0.59921413\n",
            "Iteration 838, loss = 0.59878520\n",
            "Iteration 839, loss = 0.59835595\n",
            "Iteration 840, loss = 0.59792627\n",
            "Iteration 841, loss = 0.59749702\n",
            "Iteration 842, loss = 0.59706697\n",
            "Iteration 843, loss = 0.59663658\n",
            "Iteration 844, loss = 0.59620591\n",
            "Iteration 845, loss = 0.59577548\n",
            "Iteration 846, loss = 0.59534438\n",
            "Iteration 847, loss = 0.59491304\n",
            "Iteration 848, loss = 0.59448124\n",
            "Iteration 849, loss = 0.59404914\n",
            "Iteration 850, loss = 0.59361796\n",
            "Iteration 851, loss = 0.59318468\n",
            "Iteration 852, loss = 0.59275189\n",
            "Iteration 853, loss = 0.59231867\n",
            "Iteration 854, loss = 0.59188576\n",
            "Iteration 855, loss = 0.59145208\n",
            "Iteration 856, loss = 0.59101814\n",
            "Iteration 857, loss = 0.59058380\n",
            "Iteration 858, loss = 0.59014913\n",
            "Iteration 859, loss = 0.58971404\n",
            "Iteration 860, loss = 0.58927937\n",
            "Iteration 861, loss = 0.58884368\n",
            "Iteration 862, loss = 0.58840786\n",
            "Iteration 863, loss = 0.58797175\n",
            "Iteration 864, loss = 0.58753524\n",
            "Iteration 865, loss = 0.58709840\n",
            "Iteration 866, loss = 0.58666176\n",
            "Iteration 867, loss = 0.58622422\n",
            "Iteration 868, loss = 0.58578655\n",
            "Iteration 869, loss = 0.58534857\n",
            "Iteration 870, loss = 0.58491016\n",
            "Iteration 871, loss = 0.58447137\n",
            "Iteration 872, loss = 0.58403235\n",
            "Iteration 873, loss = 0.58359291\n",
            "Iteration 874, loss = 0.58315398\n",
            "Iteration 875, loss = 0.58271350\n",
            "Iteration 876, loss = 0.58227321\n",
            "Iteration 877, loss = 0.58183263\n",
            "Iteration 878, loss = 0.58139166\n",
            "Iteration 879, loss = 0.58095025\n",
            "Iteration 880, loss = 0.58050851\n",
            "Iteration 881, loss = 0.58006647\n",
            "Iteration 882, loss = 0.57962412\n",
            "Iteration 883, loss = 0.57918134\n",
            "Iteration 884, loss = 0.57873841\n",
            "Iteration 885, loss = 0.57829510\n",
            "Iteration 886, loss = 0.57785146\n",
            "Iteration 887, loss = 0.57740739\n",
            "Iteration 888, loss = 0.57696295\n",
            "Iteration 889, loss = 0.57651813\n",
            "Iteration 890, loss = 0.57607303\n",
            "Iteration 891, loss = 0.57562750\n",
            "Iteration 892, loss = 0.57518161\n",
            "Iteration 893, loss = 0.57473539\n",
            "Iteration 894, loss = 0.57428878\n",
            "Iteration 895, loss = 0.57384181\n",
            "Iteration 896, loss = 0.57339444\n",
            "Iteration 897, loss = 0.57294683\n",
            "Iteration 898, loss = 0.57249882\n",
            "Iteration 899, loss = 0.57205042\n",
            "Iteration 900, loss = 0.57160168\n",
            "Iteration 901, loss = 0.57115262\n",
            "Iteration 902, loss = 0.57070321\n",
            "Iteration 903, loss = 0.57025327\n",
            "Iteration 904, loss = 0.56980311\n",
            "Iteration 905, loss = 0.56935250\n",
            "Iteration 906, loss = 0.56890154\n",
            "Iteration 907, loss = 0.56845027\n",
            "Iteration 908, loss = 0.56799858\n",
            "Iteration 909, loss = 0.56754651\n",
            "Iteration 910, loss = 0.56709403\n",
            "Iteration 911, loss = 0.56664122\n",
            "Iteration 912, loss = 0.56618804\n",
            "Iteration 913, loss = 0.56573445\n",
            "Iteration 914, loss = 0.56528048\n",
            "Iteration 915, loss = 0.56482614\n",
            "Iteration 916, loss = 0.56437134\n",
            "Iteration 917, loss = 0.56391623\n",
            "Iteration 918, loss = 0.56346067\n",
            "Iteration 919, loss = 0.56300472\n",
            "Iteration 920, loss = 0.56254837\n",
            "Iteration 921, loss = 0.56209169\n",
            "Iteration 922, loss = 0.56163464\n",
            "Iteration 923, loss = 0.56117712\n",
            "Iteration 924, loss = 0.56071923\n",
            "Iteration 925, loss = 0.56026091\n",
            "Iteration 926, loss = 0.55980216\n",
            "Iteration 927, loss = 0.55934308\n",
            "Iteration 928, loss = 0.55888358\n",
            "Iteration 929, loss = 0.55842369\n",
            "Iteration 930, loss = 0.55796331\n",
            "Iteration 931, loss = 0.55750256\n",
            "Iteration 932, loss = 0.55704140\n",
            "Iteration 933, loss = 0.55657986\n",
            "Iteration 934, loss = 0.55611794\n",
            "Iteration 935, loss = 0.55565550\n",
            "Iteration 936, loss = 0.55519274\n",
            "Iteration 937, loss = 0.55472955\n",
            "Iteration 938, loss = 0.55426588\n",
            "Iteration 939, loss = 0.55380187\n",
            "Iteration 940, loss = 0.55333744\n",
            "Iteration 941, loss = 0.55287252\n",
            "Iteration 942, loss = 0.55240729\n",
            "Iteration 943, loss = 0.55194162\n",
            "Iteration 944, loss = 0.55147562\n",
            "Iteration 945, loss = 0.55100925\n",
            "Iteration 946, loss = 0.55054251\n",
            "Iteration 947, loss = 0.55007530\n",
            "Iteration 948, loss = 0.54960771\n",
            "Iteration 949, loss = 0.54913956\n",
            "Iteration 950, loss = 0.54867116\n",
            "Iteration 951, loss = 0.54820226\n",
            "Iteration 952, loss = 0.54773297\n",
            "Iteration 953, loss = 0.54726326\n",
            "Iteration 954, loss = 0.54679309\n",
            "Iteration 955, loss = 0.54632250\n",
            "Iteration 956, loss = 0.54585155\n",
            "Iteration 957, loss = 0.54538010\n",
            "Iteration 958, loss = 0.54490826\n",
            "Iteration 959, loss = 0.54443600\n",
            "Iteration 960, loss = 0.54396336\n",
            "Iteration 961, loss = 0.54349021\n",
            "Iteration 962, loss = 0.54301670\n",
            "Iteration 963, loss = 0.54254268\n",
            "Iteration 964, loss = 0.54206829\n",
            "Iteration 965, loss = 0.54159348\n",
            "Iteration 966, loss = 0.54111825\n",
            "Iteration 967, loss = 0.54064251\n",
            "Iteration 968, loss = 0.54016647\n",
            "Iteration 969, loss = 0.53968991\n",
            "Iteration 970, loss = 0.53921292\n",
            "Iteration 971, loss = 0.53873556\n",
            "Iteration 972, loss = 0.53825769\n",
            "Iteration 973, loss = 0.53777950\n",
            "Iteration 974, loss = 0.53730074\n",
            "Iteration 975, loss = 0.53682165\n",
            "Iteration 976, loss = 0.53634210\n",
            "Iteration 977, loss = 0.53586216\n",
            "Iteration 978, loss = 0.53538171\n",
            "Iteration 979, loss = 0.53490089\n",
            "Iteration 980, loss = 0.53441961\n",
            "Iteration 981, loss = 0.53393794\n",
            "Iteration 982, loss = 0.53345580\n",
            "Iteration 983, loss = 0.53297325\n",
            "Iteration 984, loss = 0.53249025\n",
            "Iteration 985, loss = 0.53200690\n",
            "Iteration 986, loss = 0.53152309\n",
            "Iteration 987, loss = 0.53103885\n",
            "Iteration 988, loss = 0.53055417\n",
            "Iteration 989, loss = 0.53007047\n",
            "Iteration 990, loss = 0.52958471\n",
            "Iteration 991, loss = 0.52909992\n",
            "Iteration 992, loss = 0.52861432\n",
            "Iteration 993, loss = 0.52812820\n",
            "Iteration 994, loss = 0.52764253\n",
            "Iteration 995, loss = 0.52715551\n",
            "Iteration 996, loss = 0.52666812\n",
            "Iteration 997, loss = 0.52618194\n",
            "Iteration 998, loss = 0.52569313\n",
            "Iteration 999, loss = 0.52520516\n",
            "Iteration 1000, loss = 0.52471718\n",
            "Iteration 1001, loss = 0.52422805\n",
            "Iteration 1002, loss = 0.52373847\n",
            "Iteration 1003, loss = 0.52324968\n",
            "Iteration 1004, loss = 0.52275932\n",
            "Iteration 1005, loss = 0.52226888\n",
            "Iteration 1006, loss = 0.52177932\n",
            "Iteration 1007, loss = 0.52128838\n",
            "Iteration 1008, loss = 0.52079683\n",
            "Iteration 1009, loss = 0.52030535\n",
            "Iteration 1010, loss = 0.51981363\n",
            "Iteration 1011, loss = 0.51932105\n",
            "Iteration 1012, loss = 0.51882937\n",
            "Iteration 1013, loss = 0.51833567\n",
            "Iteration 1014, loss = 0.51784257\n",
            "Iteration 1015, loss = 0.51734946\n",
            "Iteration 1016, loss = 0.51685514\n",
            "Iteration 1017, loss = 0.51636099\n",
            "Iteration 1018, loss = 0.51586647\n",
            "Iteration 1019, loss = 0.51537125\n",
            "Iteration 1020, loss = 0.51487656\n",
            "Iteration 1021, loss = 0.51438029\n",
            "Iteration 1022, loss = 0.51388522\n",
            "Iteration 1023, loss = 0.51338803\n",
            "Iteration 1024, loss = 0.51289250\n",
            "Iteration 1025, loss = 0.51239503\n",
            "Iteration 1026, loss = 0.51189755\n",
            "Iteration 1027, loss = 0.51139992\n",
            "Iteration 1028, loss = 0.51090224\n",
            "Iteration 1029, loss = 0.51040370\n",
            "Iteration 1030, loss = 0.50990545\n",
            "Iteration 1031, loss = 0.50940635\n",
            "Iteration 1032, loss = 0.50890737\n",
            "Iteration 1033, loss = 0.50840759\n",
            "Iteration 1034, loss = 0.50790802\n",
            "Iteration 1035, loss = 0.50740760\n",
            "Iteration 1036, loss = 0.50690753\n",
            "Iteration 1037, loss = 0.50640622\n",
            "Iteration 1038, loss = 0.50590588\n",
            "Iteration 1039, loss = 0.50540383\n",
            "Iteration 1040, loss = 0.50490322\n",
            "Iteration 1041, loss = 0.50440040\n",
            "Iteration 1042, loss = 0.50389875\n",
            "Iteration 1043, loss = 0.50339572\n",
            "Iteration 1044, loss = 0.50289273\n",
            "Iteration 1045, loss = 0.50238980\n",
            "Iteration 1046, loss = 0.50188582\n",
            "Iteration 1047, loss = 0.50138299\n",
            "Iteration 1048, loss = 0.50087843\n",
            "Iteration 1049, loss = 0.50037426\n",
            "Iteration 1050, loss = 0.49986947\n",
            "Iteration 1051, loss = 0.49936448\n",
            "Iteration 1052, loss = 0.49885994\n",
            "Iteration 1053, loss = 0.49835407\n",
            "Iteration 1054, loss = 0.49784836\n",
            "Iteration 1055, loss = 0.49734260\n",
            "Iteration 1056, loss = 0.49683597\n",
            "Iteration 1057, loss = 0.49633017\n",
            "Iteration 1058, loss = 0.49582287\n",
            "Iteration 1059, loss = 0.49531553\n",
            "Iteration 1060, loss = 0.49480913\n",
            "Iteration 1061, loss = 0.49430138\n",
            "Iteration 1062, loss = 0.49379311\n",
            "Iteration 1063, loss = 0.49328559\n",
            "Iteration 1064, loss = 0.49277694\n",
            "Iteration 1065, loss = 0.49226797\n",
            "Iteration 1066, loss = 0.49176010\n",
            "Iteration 1067, loss = 0.49125055\n",
            "Iteration 1068, loss = 0.49074107\n",
            "Iteration 1069, loss = 0.49023188\n",
            "Iteration 1070, loss = 0.48972230\n",
            "Iteration 1071, loss = 0.48921218\n",
            "Iteration 1072, loss = 0.48870171\n",
            "Iteration 1073, loss = 0.48819158\n",
            "Iteration 1074, loss = 0.48768097\n",
            "Iteration 1075, loss = 0.48716985\n",
            "Iteration 1076, loss = 0.48665856\n",
            "Iteration 1077, loss = 0.48614789\n",
            "Iteration 1078, loss = 0.48563609\n",
            "Iteration 1079, loss = 0.48512431\n",
            "Iteration 1080, loss = 0.48461216\n",
            "Iteration 1081, loss = 0.48410003\n",
            "Iteration 1082, loss = 0.48358805\n",
            "Iteration 1083, loss = 0.48307551\n",
            "Iteration 1084, loss = 0.48256263\n",
            "Iteration 1085, loss = 0.48204951\n",
            "Iteration 1086, loss = 0.48153618\n",
            "Iteration 1087, loss = 0.48102409\n",
            "Iteration 1088, loss = 0.48050968\n",
            "Iteration 1089, loss = 0.47999596\n",
            "Iteration 1090, loss = 0.47948201\n",
            "Iteration 1091, loss = 0.47896791\n",
            "Iteration 1092, loss = 0.47845377\n",
            "Iteration 1093, loss = 0.47793973\n",
            "Iteration 1094, loss = 0.47742524\n",
            "Iteration 1095, loss = 0.47691046\n",
            "Iteration 1096, loss = 0.47639549\n",
            "Iteration 1097, loss = 0.47588042\n",
            "Iteration 1098, loss = 0.47536509\n",
            "Iteration 1099, loss = 0.47484972\n",
            "Iteration 1100, loss = 0.47433416\n",
            "Iteration 1101, loss = 0.47381945\n",
            "Iteration 1102, loss = 0.47330297\n",
            "Iteration 1103, loss = 0.47278716\n",
            "Iteration 1104, loss = 0.47227117\n",
            "Iteration 1105, loss = 0.47175493\n",
            "Iteration 1106, loss = 0.47123857\n",
            "Iteration 1107, loss = 0.47072213\n",
            "Iteration 1108, loss = 0.47020547\n",
            "Iteration 1109, loss = 0.46968883\n",
            "Iteration 1110, loss = 0.46917208\n",
            "Iteration 1111, loss = 0.46865503\n",
            "Iteration 1112, loss = 0.46813803\n",
            "Iteration 1113, loss = 0.46762091\n",
            "Iteration 1114, loss = 0.46710368\n",
            "Iteration 1115, loss = 0.46658631\n",
            "Iteration 1116, loss = 0.46606898\n",
            "Iteration 1117, loss = 0.46555146\n",
            "Iteration 1118, loss = 0.46503379\n",
            "Iteration 1119, loss = 0.46451613\n",
            "Iteration 1120, loss = 0.46399826\n",
            "Iteration 1121, loss = 0.46348050\n",
            "Iteration 1122, loss = 0.46296257\n",
            "Iteration 1123, loss = 0.46244454\n",
            "Iteration 1124, loss = 0.46192647\n",
            "Iteration 1125, loss = 0.46140829\n",
            "Iteration 1126, loss = 0.46088999\n",
            "Iteration 1127, loss = 0.46037174\n",
            "Iteration 1128, loss = 0.45985329\n",
            "Iteration 1129, loss = 0.45933489\n",
            "Iteration 1130, loss = 0.45881633\n",
            "Iteration 1131, loss = 0.45829770\n",
            "Iteration 1132, loss = 0.45777907\n",
            "Iteration 1133, loss = 0.45726027\n",
            "Iteration 1134, loss = 0.45674157\n",
            "Iteration 1135, loss = 0.45622272\n",
            "Iteration 1136, loss = 0.45570375\n",
            "Iteration 1137, loss = 0.45518485\n",
            "Iteration 1138, loss = 0.45466585\n",
            "Iteration 1139, loss = 0.45414675\n",
            "Iteration 1140, loss = 0.45362775\n",
            "Iteration 1141, loss = 0.45310858\n",
            "Iteration 1142, loss = 0.45258944\n",
            "Iteration 1143, loss = 0.45207018\n",
            "Iteration 1144, loss = 0.45155091\n",
            "Iteration 1145, loss = 0.45103168\n",
            "Iteration 1146, loss = 0.45051232\n",
            "Iteration 1147, loss = 0.44999302\n",
            "Iteration 1148, loss = 0.44947364\n",
            "Iteration 1149, loss = 0.44895431\n",
            "Iteration 1150, loss = 0.44843484\n",
            "Iteration 1151, loss = 0.44791737\n",
            "Iteration 1152, loss = 0.44739670\n",
            "Iteration 1153, loss = 0.44687821\n",
            "Iteration 1154, loss = 0.44635971\n",
            "Iteration 1155, loss = 0.44584083\n",
            "Iteration 1156, loss = 0.44532168\n",
            "Iteration 1157, loss = 0.44480247\n",
            "Iteration 1158, loss = 0.44428514\n",
            "Iteration 1159, loss = 0.44376503\n",
            "Iteration 1160, loss = 0.44324702\n",
            "Iteration 1161, loss = 0.44272846\n",
            "Iteration 1162, loss = 0.44220989\n",
            "Iteration 1163, loss = 0.44169109\n",
            "Iteration 1164, loss = 0.44117345\n",
            "Iteration 1165, loss = 0.44065481\n",
            "Iteration 1166, loss = 0.44013648\n",
            "Iteration 1167, loss = 0.43961891\n",
            "Iteration 1168, loss = 0.43910097\n",
            "Iteration 1169, loss = 0.43858285\n",
            "Iteration 1170, loss = 0.43806509\n",
            "Iteration 1171, loss = 0.43754791\n",
            "Iteration 1172, loss = 0.43703005\n",
            "Iteration 1173, loss = 0.43651254\n",
            "Iteration 1174, loss = 0.43599562\n",
            "Iteration 1175, loss = 0.43547800\n",
            "Iteration 1176, loss = 0.43496167\n",
            "Iteration 1177, loss = 0.43444401\n",
            "Iteration 1178, loss = 0.43392771\n",
            "Iteration 1179, loss = 0.43341092\n",
            "Iteration 1180, loss = 0.43289409\n",
            "Iteration 1181, loss = 0.43237851\n",
            "Iteration 1182, loss = 0.43186167\n",
            "Iteration 1183, loss = 0.43134637\n",
            "Iteration 1184, loss = 0.43082974\n",
            "Iteration 1185, loss = 0.43031475\n",
            "Iteration 1186, loss = 0.42979852\n",
            "Iteration 1187, loss = 0.42928370\n",
            "Iteration 1188, loss = 0.42876782\n",
            "Iteration 1189, loss = 0.42825358\n",
            "Iteration 1190, loss = 0.42773784\n",
            "Iteration 1191, loss = 0.42722422\n",
            "Iteration 1192, loss = 0.42670918\n",
            "Iteration 1193, loss = 0.42619466\n",
            "Iteration 1194, loss = 0.42568058\n",
            "Iteration 1195, loss = 0.42516651\n",
            "Iteration 1196, loss = 0.42465322\n",
            "Iteration 1197, loss = 0.42413919\n",
            "Iteration 1198, loss = 0.42362677\n",
            "Iteration 1199, loss = 0.42311282\n",
            "Iteration 1200, loss = 0.42260046\n",
            "Iteration 1201, loss = 0.42208779\n",
            "Iteration 1202, loss = 0.42157500\n",
            "Iteration 1203, loss = 0.42106320\n",
            "Iteration 1204, loss = 0.42055116\n",
            "Iteration 1205, loss = 0.42003910\n",
            "Iteration 1206, loss = 0.41952829\n",
            "Iteration 1207, loss = 0.41901669\n",
            "Iteration 1208, loss = 0.41850539\n",
            "Iteration 1209, loss = 0.41799523\n",
            "Iteration 1210, loss = 0.41748461\n",
            "Iteration 1211, loss = 0.41697418\n",
            "Iteration 1212, loss = 0.41646401\n",
            "Iteration 1213, loss = 0.41595499\n",
            "Iteration 1214, loss = 0.41544554\n",
            "Iteration 1215, loss = 0.41493610\n",
            "Iteration 1216, loss = 0.41442691\n",
            "Iteration 1217, loss = 0.41391942\n",
            "Iteration 1218, loss = 0.41341031\n",
            "Iteration 1219, loss = 0.41290221\n",
            "Iteration 1220, loss = 0.41239441\n",
            "Iteration 1221, loss = 0.41188753\n",
            "Iteration 1222, loss = 0.41138033\n",
            "Iteration 1223, loss = 0.41087356\n",
            "Iteration 1224, loss = 0.41036693\n",
            "Iteration 1225, loss = 0.40986071\n",
            "Iteration 1226, loss = 0.40935463\n",
            "Iteration 1227, loss = 0.40884932\n",
            "Iteration 1228, loss = 0.40834415\n",
            "Iteration 1229, loss = 0.40783926\n",
            "Iteration 1230, loss = 0.40733466\n",
            "Iteration 1231, loss = 0.40683039\n",
            "Iteration 1232, loss = 0.40632640\n",
            "Iteration 1233, loss = 0.40582286\n",
            "Iteration 1234, loss = 0.40531963\n",
            "Iteration 1235, loss = 0.40481670\n",
            "Iteration 1236, loss = 0.40431426\n",
            "Iteration 1237, loss = 0.40381244\n",
            "Iteration 1238, loss = 0.40331036\n",
            "Iteration 1239, loss = 0.40280899\n",
            "Iteration 1240, loss = 0.40230796\n",
            "Iteration 1241, loss = 0.40180727\n",
            "Iteration 1242, loss = 0.40130690\n",
            "Iteration 1243, loss = 0.40080683\n",
            "Iteration 1244, loss = 0.40030721\n",
            "Iteration 1245, loss = 0.39980796\n",
            "Iteration 1246, loss = 0.39930913\n",
            "Iteration 1247, loss = 0.39881069\n",
            "Iteration 1248, loss = 0.39831249\n",
            "Iteration 1249, loss = 0.39781481\n",
            "Iteration 1250, loss = 0.39731751\n",
            "Iteration 1251, loss = 0.39682059\n",
            "Iteration 1252, loss = 0.39632397\n",
            "Iteration 1253, loss = 0.39582793\n",
            "Iteration 1254, loss = 0.39533221\n",
            "Iteration 1255, loss = 0.39483679\n",
            "Iteration 1256, loss = 0.39434184\n",
            "Iteration 1257, loss = 0.39384731\n",
            "Iteration 1258, loss = 0.39335316\n",
            "Iteration 1259, loss = 0.39285943\n",
            "Iteration 1260, loss = 0.39236611\n",
            "Iteration 1261, loss = 0.39187312\n",
            "Iteration 1262, loss = 0.39138064\n",
            "Iteration 1263, loss = 0.39088847\n",
            "Iteration 1264, loss = 0.39039677\n",
            "Iteration 1265, loss = 0.38990541\n",
            "Iteration 1266, loss = 0.38941455\n",
            "Iteration 1267, loss = 0.38892409\n",
            "Iteration 1268, loss = 0.38843404\n",
            "Iteration 1269, loss = 0.38794442\n",
            "Iteration 1270, loss = 0.38745510\n",
            "Iteration 1271, loss = 0.38696636\n",
            "Iteration 1272, loss = 0.38647802\n",
            "Iteration 1273, loss = 0.38599010\n",
            "Iteration 1274, loss = 0.38550257\n",
            "Iteration 1275, loss = 0.38501553\n",
            "Iteration 1276, loss = 0.38452879\n",
            "Iteration 1277, loss = 0.38404258\n",
            "Iteration 1278, loss = 0.38355678\n",
            "Iteration 1279, loss = 0.38307138\n",
            "Iteration 1280, loss = 0.38258654\n",
            "Iteration 1281, loss = 0.38210204\n",
            "Iteration 1282, loss = 0.38161802\n",
            "Iteration 1283, loss = 0.38113451\n",
            "Iteration 1284, loss = 0.38065136\n",
            "Iteration 1285, loss = 0.38016876\n",
            "Iteration 1286, loss = 0.37968648\n",
            "Iteration 1287, loss = 0.37920465\n",
            "Iteration 1288, loss = 0.37872344\n",
            "Iteration 1289, loss = 0.37824253\n",
            "Iteration 1290, loss = 0.37776218\n",
            "Iteration 1291, loss = 0.37728233\n",
            "Iteration 1292, loss = 0.37680290\n",
            "Iteration 1293, loss = 0.37632393\n",
            "Iteration 1294, loss = 0.37584545\n",
            "Iteration 1295, loss = 0.37536738\n",
            "Iteration 1296, loss = 0.37488978\n",
            "Iteration 1297, loss = 0.37441277\n",
            "Iteration 1298, loss = 0.37393616\n",
            "Iteration 1299, loss = 0.37346009\n",
            "Iteration 1300, loss = 0.37298444\n",
            "Iteration 1301, loss = 0.37250931\n",
            "Iteration 1302, loss = 0.37203473\n",
            "Iteration 1303, loss = 0.37156058\n",
            "Iteration 1304, loss = 0.37108691\n",
            "Iteration 1305, loss = 0.37061382\n",
            "Iteration 1306, loss = 0.37014118\n",
            "Iteration 1307, loss = 0.36966903\n",
            "Iteration 1308, loss = 0.36919745\n",
            "Iteration 1309, loss = 0.36872634\n",
            "Iteration 1310, loss = 0.36825570\n",
            "Iteration 1311, loss = 0.36778564\n",
            "Iteration 1312, loss = 0.36731607\n",
            "Iteration 1313, loss = 0.36684702\n",
            "Iteration 1314, loss = 0.36637858\n",
            "Iteration 1315, loss = 0.36591058\n",
            "Iteration 1316, loss = 0.36544322\n",
            "Iteration 1317, loss = 0.36497628\n",
            "Iteration 1318, loss = 0.36450989\n",
            "Iteration 1319, loss = 0.36404408\n",
            "Iteration 1320, loss = 0.36357904\n",
            "Iteration 1321, loss = 0.36311432\n",
            "Iteration 1322, loss = 0.36265020\n",
            "Iteration 1323, loss = 0.36218676\n",
            "Iteration 1324, loss = 0.36172381\n",
            "Iteration 1325, loss = 0.36126139\n",
            "Iteration 1326, loss = 0.36080154\n",
            "Iteration 1327, loss = 0.36034103\n",
            "Iteration 1328, loss = 0.35988096\n",
            "Iteration 1329, loss = 0.35942148\n",
            "Iteration 1330, loss = 0.35896279\n",
            "Iteration 1331, loss = 0.35850506\n",
            "Iteration 1332, loss = 0.35804761\n",
            "Iteration 1333, loss = 0.35759115\n",
            "Iteration 1334, loss = 0.35713486\n",
            "Iteration 1335, loss = 0.35667938\n",
            "Iteration 1336, loss = 0.35622462\n",
            "Iteration 1337, loss = 0.35577073\n",
            "Iteration 1338, loss = 0.35531722\n",
            "Iteration 1339, loss = 0.35486438\n",
            "Iteration 1340, loss = 0.35441251\n",
            "Iteration 1341, loss = 0.35396096\n",
            "Iteration 1342, loss = 0.35351061\n",
            "Iteration 1343, loss = 0.35306027\n",
            "Iteration 1344, loss = 0.35261090\n",
            "Iteration 1345, loss = 0.35216236\n",
            "Iteration 1346, loss = 0.35171424\n",
            "Iteration 1347, loss = 0.35126676\n",
            "Iteration 1348, loss = 0.35082005\n",
            "Iteration 1349, loss = 0.35037439\n",
            "Iteration 1350, loss = 0.34992984\n",
            "Iteration 1351, loss = 0.34948430\n",
            "Iteration 1352, loss = 0.34904208\n",
            "Iteration 1353, loss = 0.34859739\n",
            "Iteration 1354, loss = 0.34815633\n",
            "Iteration 1355, loss = 0.34771383\n",
            "Iteration 1356, loss = 0.34727294\n",
            "Iteration 1357, loss = 0.34683276\n",
            "Iteration 1358, loss = 0.34639331\n",
            "Iteration 1359, loss = 0.34595455\n",
            "Iteration 1360, loss = 0.34551659\n",
            "Iteration 1361, loss = 0.34507963\n",
            "Iteration 1362, loss = 0.34464299\n",
            "Iteration 1363, loss = 0.34420717\n",
            "Iteration 1364, loss = 0.34377201\n",
            "Iteration 1365, loss = 0.34333753\n",
            "Iteration 1366, loss = 0.34290378\n",
            "Iteration 1367, loss = 0.34247076\n",
            "Iteration 1368, loss = 0.34203852\n",
            "Iteration 1369, loss = 0.34160703\n",
            "Iteration 1370, loss = 0.34117616\n",
            "Iteration 1371, loss = 0.34074613\n",
            "Iteration 1372, loss = 0.34031666\n",
            "Iteration 1373, loss = 0.33988808\n",
            "Iteration 1374, loss = 0.33946015\n",
            "Iteration 1375, loss = 0.33903296\n",
            "Iteration 1376, loss = 0.33860641\n",
            "Iteration 1377, loss = 0.33818066\n",
            "Iteration 1378, loss = 0.33775555\n",
            "Iteration 1379, loss = 0.33733122\n",
            "Iteration 1380, loss = 0.33690763\n",
            "Iteration 1381, loss = 0.33648466\n",
            "Iteration 1382, loss = 0.33606248\n",
            "Iteration 1383, loss = 0.33564099\n",
            "Iteration 1384, loss = 0.33522019\n",
            "Iteration 1385, loss = 0.33480000\n",
            "Iteration 1386, loss = 0.33438074\n",
            "Iteration 1387, loss = 0.33396206\n",
            "Iteration 1388, loss = 0.33354406\n",
            "Iteration 1389, loss = 0.33312675\n",
            "Iteration 1390, loss = 0.33271017\n",
            "Iteration 1391, loss = 0.33229427\n",
            "Iteration 1392, loss = 0.33187911\n",
            "Iteration 1393, loss = 0.33146461\n",
            "Iteration 1394, loss = 0.33105084\n",
            "Iteration 1395, loss = 0.33063827\n",
            "Iteration 1396, loss = 0.33022563\n",
            "Iteration 1397, loss = 0.32981432\n",
            "Iteration 1398, loss = 0.32940370\n",
            "Iteration 1399, loss = 0.32899361\n",
            "Iteration 1400, loss = 0.32858434\n",
            "Iteration 1401, loss = 0.32817584\n",
            "Iteration 1402, loss = 0.32776798\n",
            "Iteration 1403, loss = 0.32736100\n",
            "Iteration 1404, loss = 0.32695454\n",
            "Iteration 1405, loss = 0.32654914\n",
            "Iteration 1406, loss = 0.32614409\n",
            "Iteration 1407, loss = 0.32574027\n",
            "Iteration 1408, loss = 0.32533658\n",
            "Iteration 1409, loss = 0.32493420\n",
            "Iteration 1410, loss = 0.32453218\n",
            "Iteration 1411, loss = 0.32413106\n",
            "Iteration 1412, loss = 0.32373044\n",
            "Iteration 1413, loss = 0.32333107\n",
            "Iteration 1414, loss = 0.32293187\n",
            "Iteration 1415, loss = 0.32253409\n",
            "Iteration 1416, loss = 0.32213637\n",
            "Iteration 1417, loss = 0.32173998\n",
            "Iteration 1418, loss = 0.32134388\n",
            "Iteration 1419, loss = 0.32094847\n",
            "Iteration 1420, loss = 0.32055433\n",
            "Iteration 1421, loss = 0.32016039\n",
            "Iteration 1422, loss = 0.31976743\n",
            "Iteration 1423, loss = 0.31937544\n",
            "Iteration 1424, loss = 0.31898399\n",
            "Iteration 1425, loss = 0.31859325\n",
            "Iteration 1426, loss = 0.31820321\n",
            "Iteration 1427, loss = 0.31781402\n",
            "Iteration 1428, loss = 0.31742567\n",
            "Iteration 1429, loss = 0.31703795\n",
            "Iteration 1430, loss = 0.31665109\n",
            "Iteration 1431, loss = 0.31626492\n",
            "Iteration 1432, loss = 0.31587941\n",
            "Iteration 1433, loss = 0.31549466\n",
            "Iteration 1434, loss = 0.31511092\n",
            "Iteration 1435, loss = 0.31472768\n",
            "Iteration 1436, loss = 0.31434533\n",
            "Iteration 1437, loss = 0.31396364\n",
            "Iteration 1438, loss = 0.31358275\n",
            "Iteration 1439, loss = 0.31320260\n",
            "Iteration 1440, loss = 0.31282327\n",
            "Iteration 1441, loss = 0.31244451\n",
            "Iteration 1442, loss = 0.31206666\n",
            "Iteration 1443, loss = 0.31168954\n",
            "Iteration 1444, loss = 0.31131313\n",
            "Iteration 1445, loss = 0.31093753\n",
            "Iteration 1446, loss = 0.31056254\n",
            "Iteration 1447, loss = 0.31018843\n",
            "Iteration 1448, loss = 0.30981505\n",
            "Iteration 1449, loss = 0.30944239\n",
            "Iteration 1450, loss = 0.30907043\n",
            "Iteration 1451, loss = 0.30869922\n",
            "Iteration 1452, loss = 0.30832880\n",
            "Iteration 1453, loss = 0.30795916\n",
            "Iteration 1454, loss = 0.30759012\n",
            "Iteration 1455, loss = 0.30722193\n",
            "Iteration 1456, loss = 0.30685440\n",
            "Iteration 1457, loss = 0.30648765\n",
            "Iteration 1458, loss = 0.30612159\n",
            "Iteration 1459, loss = 0.30575638\n",
            "Iteration 1460, loss = 0.30539182\n",
            "Iteration 1461, loss = 0.30502789\n",
            "Iteration 1462, loss = 0.30466480\n",
            "Iteration 1463, loss = 0.30430237\n",
            "Iteration 1464, loss = 0.30394077\n",
            "Iteration 1465, loss = 0.30357978\n",
            "Iteration 1466, loss = 0.30321957\n",
            "Iteration 1467, loss = 0.30286014\n",
            "Iteration 1468, loss = 0.30250134\n",
            "Iteration 1469, loss = 0.30214329\n",
            "Iteration 1470, loss = 0.30178603\n",
            "Iteration 1471, loss = 0.30142936\n",
            "Iteration 1472, loss = 0.30107349\n",
            "Iteration 1473, loss = 0.30071840\n",
            "Iteration 1474, loss = 0.30036383\n",
            "Iteration 1475, loss = 0.30001028\n",
            "Iteration 1476, loss = 0.29965721\n",
            "Iteration 1477, loss = 0.29930488\n",
            "Iteration 1478, loss = 0.29895342\n",
            "Iteration 1479, loss = 0.29860251\n",
            "Iteration 1480, loss = 0.29825244\n",
            "Iteration 1481, loss = 0.29790300\n",
            "Iteration 1482, loss = 0.29755438\n",
            "Iteration 1483, loss = 0.29720648\n",
            "Iteration 1484, loss = 0.29685915\n",
            "Iteration 1485, loss = 0.29651260\n",
            "Iteration 1486, loss = 0.29616672\n",
            "Iteration 1487, loss = 0.29582158\n",
            "Iteration 1488, loss = 0.29547725\n",
            "Iteration 1489, loss = 0.29513360\n",
            "Iteration 1490, loss = 0.29479052\n",
            "Iteration 1491, loss = 0.29444832\n",
            "Iteration 1492, loss = 0.29410672\n",
            "Iteration 1493, loss = 0.29376598\n",
            "Iteration 1494, loss = 0.29342587\n",
            "Iteration 1495, loss = 0.29308646\n",
            "Iteration 1496, loss = 0.29274777\n",
            "Iteration 1497, loss = 0.29240974\n",
            "Iteration 1498, loss = 0.29207246\n",
            "Iteration 1499, loss = 0.29173587\n",
            "Iteration 1500, loss = 0.29140103\n",
            "Iteration 1501, loss = 0.29106657\n",
            "Iteration 1502, loss = 0.29073297\n",
            "Iteration 1503, loss = 0.29040001\n",
            "Iteration 1504, loss = 0.29006804\n",
            "Iteration 1505, loss = 0.28973690\n",
            "Iteration 1506, loss = 0.28940654\n",
            "Iteration 1507, loss = 0.28907676\n",
            "Iteration 1508, loss = 0.28874768\n",
            "Iteration 1509, loss = 0.28841949\n",
            "Iteration 1510, loss = 0.28809198\n",
            "Iteration 1511, loss = 0.28776526\n",
            "Iteration 1512, loss = 0.28743943\n",
            "Iteration 1513, loss = 0.28711443\n",
            "Iteration 1514, loss = 0.28679020\n",
            "Iteration 1515, loss = 0.28646671\n",
            "Iteration 1516, loss = 0.28614398\n",
            "Iteration 1517, loss = 0.28582209\n",
            "Iteration 1518, loss = 0.28550090\n",
            "Iteration 1519, loss = 0.28518065\n",
            "Iteration 1520, loss = 0.28486103\n",
            "Iteration 1521, loss = 0.28454216\n",
            "Iteration 1522, loss = 0.28422407\n",
            "Iteration 1523, loss = 0.28390678\n",
            "Iteration 1524, loss = 0.28359022\n",
            "Iteration 1525, loss = 0.28327441\n",
            "Iteration 1526, loss = 0.28295931\n",
            "Iteration 1527, loss = 0.28264505\n",
            "Iteration 1528, loss = 0.28233143\n",
            "Iteration 1529, loss = 0.28201861\n",
            "Iteration 1530, loss = 0.28170649\n",
            "Iteration 1531, loss = 0.28139518\n",
            "Iteration 1532, loss = 0.28108449\n",
            "Iteration 1533, loss = 0.28077457\n",
            "Iteration 1534, loss = 0.28046528\n",
            "Iteration 1535, loss = 0.28015691\n",
            "Iteration 1536, loss = 0.27984905\n",
            "Iteration 1537, loss = 0.27954192\n",
            "Iteration 1538, loss = 0.27923554\n",
            "Iteration 1539, loss = 0.27892986\n",
            "Iteration 1540, loss = 0.27862495\n",
            "Iteration 1541, loss = 0.27832063\n",
            "Iteration 1542, loss = 0.27801705\n",
            "Iteration 1543, loss = 0.27771418\n",
            "Iteration 1544, loss = 0.27741185\n",
            "Iteration 1545, loss = 0.27711047\n",
            "Iteration 1546, loss = 0.27680963\n",
            "Iteration 1547, loss = 0.27650957\n",
            "Iteration 1548, loss = 0.27621006\n",
            "Iteration 1549, loss = 0.27591130\n",
            "Iteration 1550, loss = 0.27561313\n",
            "Iteration 1551, loss = 0.27531578\n",
            "Iteration 1552, loss = 0.27501908\n",
            "Iteration 1553, loss = 0.27472302\n",
            "Iteration 1554, loss = 0.27442772\n",
            "Iteration 1555, loss = 0.27413297\n",
            "Iteration 1556, loss = 0.27383896\n",
            "Iteration 1557, loss = 0.27354563\n",
            "Iteration 1558, loss = 0.27325295\n",
            "Iteration 1559, loss = 0.27296099\n",
            "Iteration 1560, loss = 0.27266964\n",
            "Iteration 1561, loss = 0.27237902\n",
            "Iteration 1562, loss = 0.27208899\n",
            "Iteration 1563, loss = 0.27179971\n",
            "Iteration 1564, loss = 0.27151110\n",
            "Iteration 1565, loss = 0.27122312\n",
            "Iteration 1566, loss = 0.27093582\n",
            "Iteration 1567, loss = 0.27064909\n",
            "Iteration 1568, loss = 0.27036332\n",
            "Iteration 1569, loss = 0.27007796\n",
            "Iteration 1570, loss = 0.26979344\n",
            "Iteration 1571, loss = 0.26950994\n",
            "Iteration 1572, loss = 0.26922659\n",
            "Iteration 1573, loss = 0.26894410\n",
            "Iteration 1574, loss = 0.26866269\n",
            "Iteration 1575, loss = 0.26838141\n",
            "Iteration 1576, loss = 0.26810131\n",
            "Iteration 1577, loss = 0.26782146\n",
            "Iteration 1578, loss = 0.26754248\n",
            "Iteration 1579, loss = 0.26726425\n",
            "Iteration 1580, loss = 0.26698651\n",
            "Iteration 1581, loss = 0.26670961\n",
            "Iteration 1582, loss = 0.26643329\n",
            "Iteration 1583, loss = 0.26615760\n",
            "Iteration 1584, loss = 0.26588263\n",
            "Iteration 1585, loss = 0.26560824\n",
            "Iteration 1586, loss = 0.26533466\n",
            "Iteration 1587, loss = 0.26506158\n",
            "Iteration 1588, loss = 0.26478917\n",
            "Iteration 1589, loss = 0.26451744\n",
            "Iteration 1590, loss = 0.26424637\n",
            "Iteration 1591, loss = 0.26397584\n",
            "Iteration 1592, loss = 0.26370599\n",
            "Iteration 1593, loss = 0.26343674\n",
            "Iteration 1594, loss = 0.26316822\n",
            "Iteration 1595, loss = 0.26290032\n",
            "Iteration 1596, loss = 0.26263301\n",
            "Iteration 1597, loss = 0.26236640\n",
            "Iteration 1598, loss = 0.26210040\n",
            "Iteration 1599, loss = 0.26183493\n",
            "Iteration 1600, loss = 0.26157023\n",
            "Iteration 1601, loss = 0.26130611\n",
            "Iteration 1602, loss = 0.26104259\n",
            "Iteration 1603, loss = 0.26077976\n",
            "Iteration 1604, loss = 0.26051748\n",
            "Iteration 1605, loss = 0.26025589\n",
            "Iteration 1606, loss = 0.25999495\n",
            "Iteration 1607, loss = 0.25973449\n",
            "Iteration 1608, loss = 0.25947484\n",
            "Iteration 1609, loss = 0.25921574\n",
            "Iteration 1610, loss = 0.25895721\n",
            "Iteration 1611, loss = 0.25869934\n",
            "Iteration 1612, loss = 0.25844202\n",
            "Iteration 1613, loss = 0.25818540\n",
            "Iteration 1614, loss = 0.25792945\n",
            "Iteration 1615, loss = 0.25767402\n",
            "Iteration 1616, loss = 0.25741922\n",
            "Iteration 1617, loss = 0.25716511\n",
            "Iteration 1618, loss = 0.25691148\n",
            "Iteration 1619, loss = 0.25665857\n",
            "Iteration 1620, loss = 0.25640624\n",
            "Iteration 1621, loss = 0.25615451\n",
            "Iteration 1622, loss = 0.25590338\n",
            "Iteration 1623, loss = 0.25565289\n",
            "Iteration 1624, loss = 0.25540299\n",
            "Iteration 1625, loss = 0.25515368\n",
            "Iteration 1626, loss = 0.25490495\n",
            "Iteration 1627, loss = 0.25465695\n",
            "Iteration 1628, loss = 0.25440944\n",
            "Iteration 1629, loss = 0.25416254\n",
            "Iteration 1630, loss = 0.25391623\n",
            "Iteration 1631, loss = 0.25367060\n",
            "Iteration 1632, loss = 0.25342555\n",
            "Iteration 1633, loss = 0.25318105\n",
            "Iteration 1634, loss = 0.25293708\n",
            "Iteration 1635, loss = 0.25269377\n",
            "Iteration 1636, loss = 0.25245112\n",
            "Iteration 1637, loss = 0.25220904\n",
            "Iteration 1638, loss = 0.25196749\n",
            "Iteration 1639, loss = 0.25172659\n",
            "Iteration 1640, loss = 0.25148629\n",
            "Iteration 1641, loss = 0.25124652\n",
            "Iteration 1642, loss = 0.25100737\n",
            "Iteration 1643, loss = 0.25076877\n",
            "Iteration 1644, loss = 0.25053076\n",
            "Iteration 1645, loss = 0.25029344\n",
            "Iteration 1646, loss = 0.25005658\n",
            "Iteration 1647, loss = 0.24982032\n",
            "Iteration 1648, loss = 0.24958466\n",
            "Iteration 1649, loss = 0.24934959\n",
            "Iteration 1650, loss = 0.24911513\n",
            "Iteration 1651, loss = 0.24888126\n",
            "Iteration 1652, loss = 0.24864791\n",
            "Iteration 1653, loss = 0.24841512\n",
            "Iteration 1654, loss = 0.24818301\n",
            "Iteration 1655, loss = 0.24795131\n",
            "Iteration 1656, loss = 0.24772029\n",
            "Iteration 1657, loss = 0.24748981\n",
            "Iteration 1658, loss = 0.24725993\n",
            "Iteration 1659, loss = 0.24703061\n",
            "Iteration 1660, loss = 0.24680180\n",
            "Iteration 1661, loss = 0.24657360\n",
            "Iteration 1662, loss = 0.24634597\n",
            "Iteration 1663, loss = 0.24611889\n",
            "Iteration 1664, loss = 0.24589249\n",
            "Iteration 1665, loss = 0.24566648\n",
            "Iteration 1666, loss = 0.24544110\n",
            "Iteration 1667, loss = 0.24521630\n",
            "Iteration 1668, loss = 0.24499207\n",
            "Iteration 1669, loss = 0.24476831\n",
            "Iteration 1670, loss = 0.24454525\n",
            "Iteration 1671, loss = 0.24432260\n",
            "Iteration 1672, loss = 0.24410057\n",
            "Iteration 1673, loss = 0.24387907\n",
            "Iteration 1674, loss = 0.24365817\n",
            "Iteration 1675, loss = 0.24343775\n",
            "Iteration 1676, loss = 0.24321791\n",
            "Iteration 1677, loss = 0.24299865\n",
            "Iteration 1678, loss = 0.24277987\n",
            "Iteration 1679, loss = 0.24256168\n",
            "Iteration 1680, loss = 0.24234408\n",
            "Iteration 1681, loss = 0.24212695\n",
            "Iteration 1682, loss = 0.24191039\n",
            "Iteration 1683, loss = 0.24169429\n",
            "Iteration 1684, loss = 0.24147892\n",
            "Iteration 1685, loss = 0.24126393\n",
            "Iteration 1686, loss = 0.24104955\n",
            "Iteration 1687, loss = 0.24083562\n",
            "Iteration 1688, loss = 0.24062239\n",
            "Iteration 1689, loss = 0.24040965\n",
            "Iteration 1690, loss = 0.24019732\n",
            "Iteration 1691, loss = 0.23998554\n",
            "Iteration 1692, loss = 0.23977437\n",
            "Iteration 1693, loss = 0.23956377\n",
            "Iteration 1694, loss = 0.23935358\n",
            "Iteration 1695, loss = 0.23914399\n",
            "Iteration 1696, loss = 0.23893495\n",
            "Iteration 1697, loss = 0.23872642\n",
            "Iteration 1698, loss = 0.23851839\n",
            "Iteration 1699, loss = 0.23831082\n",
            "Iteration 1700, loss = 0.23810389\n",
            "Iteration 1701, loss = 0.23789748\n",
            "Iteration 1702, loss = 0.23769142\n",
            "Iteration 1703, loss = 0.23748605\n",
            "Iteration 1704, loss = 0.23728114\n",
            "Iteration 1705, loss = 0.23707679\n",
            "Iteration 1706, loss = 0.23687286\n",
            "Iteration 1707, loss = 0.23666955\n",
            "Iteration 1708, loss = 0.23646669\n",
            "Iteration 1709, loss = 0.23626436\n",
            "Iteration 1710, loss = 0.23606253\n",
            "Iteration 1711, loss = 0.23586121\n",
            "Iteration 1712, loss = 0.23566036\n",
            "Iteration 1713, loss = 0.23546015\n",
            "Iteration 1714, loss = 0.23526027\n",
            "Iteration 1715, loss = 0.23506103\n",
            "Iteration 1716, loss = 0.23486219\n",
            "Iteration 1717, loss = 0.23466388\n",
            "Iteration 1718, loss = 0.23446620\n",
            "Iteration 1719, loss = 0.23426889\n",
            "Iteration 1720, loss = 0.23407212\n",
            "Iteration 1721, loss = 0.23387580\n",
            "Iteration 1722, loss = 0.23368010\n",
            "Iteration 1723, loss = 0.23348470\n",
            "Iteration 1724, loss = 0.23329001\n",
            "Iteration 1725, loss = 0.23309560\n",
            "Iteration 1726, loss = 0.23290186\n",
            "Iteration 1727, loss = 0.23270856\n",
            "Iteration 1728, loss = 0.23251567\n",
            "Iteration 1729, loss = 0.23232333\n",
            "Iteration 1730, loss = 0.23213146\n",
            "Iteration 1731, loss = 0.23194008\n",
            "Iteration 1732, loss = 0.23174922\n",
            "Iteration 1733, loss = 0.23155888\n",
            "Iteration 1734, loss = 0.23136894\n",
            "Iteration 1735, loss = 0.23117947\n",
            "Iteration 1736, loss = 0.23099056\n",
            "Iteration 1737, loss = 0.23080207\n",
            "Iteration 1738, loss = 0.23061405\n",
            "Iteration 1739, loss = 0.23042650\n",
            "Iteration 1740, loss = 0.23023950\n",
            "Iteration 1741, loss = 0.23005302\n",
            "Iteration 1742, loss = 0.22986684\n",
            "Iteration 1743, loss = 0.22968121\n",
            "Iteration 1744, loss = 0.22949601\n",
            "Iteration 1745, loss = 0.22931143\n",
            "Iteration 1746, loss = 0.22912718\n",
            "Iteration 1747, loss = 0.22894345\n",
            "Iteration 1748, loss = 0.22876014\n",
            "Iteration 1749, loss = 0.22857740\n",
            "Iteration 1750, loss = 0.22839501\n",
            "Iteration 1751, loss = 0.22821321\n",
            "Iteration 1752, loss = 0.22803177\n",
            "Iteration 1753, loss = 0.22785079\n",
            "Iteration 1754, loss = 0.22767025\n",
            "Iteration 1755, loss = 0.22749024\n",
            "Iteration 1756, loss = 0.22731070\n",
            "Iteration 1757, loss = 0.22713159\n",
            "Iteration 1758, loss = 0.22695287\n",
            "Iteration 1759, loss = 0.22677456\n",
            "Iteration 1760, loss = 0.22659687\n",
            "Iteration 1761, loss = 0.22641958\n",
            "Iteration 1762, loss = 0.22624266\n",
            "Iteration 1763, loss = 0.22606624\n",
            "Iteration 1764, loss = 0.22589022\n",
            "Iteration 1765, loss = 0.22571480\n",
            "Iteration 1766, loss = 0.22553964\n",
            "Iteration 1767, loss = 0.22536508\n",
            "Iteration 1768, loss = 0.22519079\n",
            "Iteration 1769, loss = 0.22501713\n",
            "Iteration 1770, loss = 0.22484380\n",
            "Iteration 1771, loss = 0.22467096\n",
            "Iteration 1772, loss = 0.22449857\n",
            "Iteration 1773, loss = 0.22432648\n",
            "Iteration 1774, loss = 0.22415495\n",
            "Iteration 1775, loss = 0.22398383\n",
            "Iteration 1776, loss = 0.22381317\n",
            "Iteration 1777, loss = 0.22364292\n",
            "Iteration 1778, loss = 0.22347318\n",
            "Iteration 1779, loss = 0.22330376\n",
            "Iteration 1780, loss = 0.22313476\n",
            "Iteration 1781, loss = 0.22296625\n",
            "Iteration 1782, loss = 0.22279816\n",
            "Iteration 1783, loss = 0.22263058\n",
            "Iteration 1784, loss = 0.22246330\n",
            "Iteration 1785, loss = 0.22229642\n",
            "Iteration 1786, loss = 0.22213006\n",
            "Iteration 1787, loss = 0.22196412\n",
            "Iteration 1788, loss = 0.22179854\n",
            "Iteration 1789, loss = 0.22163346\n",
            "Iteration 1790, loss = 0.22146866\n",
            "Iteration 1791, loss = 0.22130441\n",
            "Iteration 1792, loss = 0.22114055\n",
            "Iteration 1793, loss = 0.22097705\n",
            "Iteration 1794, loss = 0.22081401\n",
            "Iteration 1795, loss = 0.22065133\n",
            "Iteration 1796, loss = 0.22048910\n",
            "Iteration 1797, loss = 0.22032733\n",
            "Iteration 1798, loss = 0.22016599\n",
            "Iteration 1799, loss = 0.22000494\n",
            "Iteration 1800, loss = 0.21984435\n",
            "Iteration 1801, loss = 0.21968420\n",
            "Iteration 1802, loss = 0.21952445\n",
            "Iteration 1803, loss = 0.21936505\n",
            "Iteration 1804, loss = 0.21920611\n",
            "Iteration 1805, loss = 0.21904752\n",
            "Iteration 1806, loss = 0.21888931\n",
            "Iteration 1807, loss = 0.21873164\n",
            "Iteration 1808, loss = 0.21857422\n",
            "Iteration 1809, loss = 0.21841726\n",
            "Iteration 1810, loss = 0.21826076\n",
            "Iteration 1811, loss = 0.21810456\n",
            "Iteration 1812, loss = 0.21794874\n",
            "Iteration 1813, loss = 0.21779342\n",
            "Iteration 1814, loss = 0.21763840\n",
            "Iteration 1815, loss = 0.21748382\n",
            "Iteration 1816, loss = 0.21732961\n",
            "Iteration 1817, loss = 0.21717587\n",
            "Iteration 1818, loss = 0.21702239\n",
            "Iteration 1819, loss = 0.21686936\n",
            "Iteration 1820, loss = 0.21671672\n",
            "Iteration 1821, loss = 0.21656445\n",
            "Iteration 1822, loss = 0.21641251\n",
            "Iteration 1823, loss = 0.21626109\n",
            "Iteration 1824, loss = 0.21610996\n",
            "Iteration 1825, loss = 0.21595922\n",
            "Iteration 1826, loss = 0.21580887\n",
            "Iteration 1827, loss = 0.21565890\n",
            "Iteration 1828, loss = 0.21550931\n",
            "Iteration 1829, loss = 0.21536003\n",
            "Iteration 1830, loss = 0.21521122\n",
            "Iteration 1831, loss = 0.21506271\n",
            "Iteration 1832, loss = 0.21491466\n",
            "Iteration 1833, loss = 0.21476701\n",
            "Iteration 1834, loss = 0.21461969\n",
            "Iteration 1835, loss = 0.21447256\n",
            "Iteration 1836, loss = 0.21432603\n",
            "Iteration 1837, loss = 0.21417976\n",
            "Iteration 1838, loss = 0.21403388\n",
            "Iteration 1839, loss = 0.21388842\n",
            "Iteration 1840, loss = 0.21374325\n",
            "Iteration 1841, loss = 0.21359845\n",
            "Iteration 1842, loss = 0.21345406\n",
            "Iteration 1843, loss = 0.21331007\n",
            "Iteration 1844, loss = 0.21316638\n",
            "Iteration 1845, loss = 0.21302301\n",
            "Iteration 1846, loss = 0.21288004\n",
            "Iteration 1847, loss = 0.21273747\n",
            "Iteration 1848, loss = 0.21259515\n",
            "Iteration 1849, loss = 0.21245331\n",
            "Iteration 1850, loss = 0.21231178\n",
            "Iteration 1851, loss = 0.21217061\n",
            "Iteration 1852, loss = 0.21202976\n",
            "Iteration 1853, loss = 0.21188931\n",
            "Iteration 1854, loss = 0.21174919\n",
            "Iteration 1855, loss = 0.21160943\n",
            "Iteration 1856, loss = 0.21146998\n",
            "Iteration 1857, loss = 0.21133091\n",
            "Iteration 1858, loss = 0.21119215\n",
            "Iteration 1859, loss = 0.21105381\n",
            "Iteration 1860, loss = 0.21091576\n",
            "Iteration 1861, loss = 0.21077804\n",
            "Iteration 1862, loss = 0.21064074\n",
            "Iteration 1863, loss = 0.21050371\n",
            "Iteration 1864, loss = 0.21036709\n",
            "Iteration 1865, loss = 0.21023078\n",
            "Iteration 1866, loss = 0.21009482\n",
            "Iteration 1867, loss = 0.20995917\n",
            "Iteration 1868, loss = 0.20982466\n",
            "Iteration 1869, loss = 0.20969028\n",
            "Iteration 1870, loss = 0.20955631\n",
            "Iteration 1871, loss = 0.20942277\n",
            "Iteration 1872, loss = 0.20928959\n",
            "Iteration 1873, loss = 0.20915688\n",
            "Iteration 1874, loss = 0.20902445\n",
            "Iteration 1875, loss = 0.20889238\n",
            "Iteration 1876, loss = 0.20876052\n",
            "Iteration 1877, loss = 0.20862907\n",
            "Iteration 1878, loss = 0.20849788\n",
            "Iteration 1879, loss = 0.20836722\n",
            "Iteration 1880, loss = 0.20823679\n",
            "Iteration 1881, loss = 0.20810686\n",
            "Iteration 1882, loss = 0.20797720\n",
            "Iteration 1883, loss = 0.20784798\n",
            "Iteration 1884, loss = 0.20771910\n",
            "Iteration 1885, loss = 0.20759055\n",
            "Iteration 1886, loss = 0.20746230\n",
            "Iteration 1887, loss = 0.20733447\n",
            "Iteration 1888, loss = 0.20720699\n",
            "Iteration 1889, loss = 0.20707985\n",
            "Iteration 1890, loss = 0.20695297\n",
            "Iteration 1891, loss = 0.20682658\n",
            "Iteration 1892, loss = 0.20670044\n",
            "Iteration 1893, loss = 0.20657461\n",
            "Iteration 1894, loss = 0.20644923\n",
            "Iteration 1895, loss = 0.20632410\n",
            "Iteration 1896, loss = 0.20619924\n",
            "Iteration 1897, loss = 0.20607474\n",
            "Iteration 1898, loss = 0.20595061\n",
            "Iteration 1899, loss = 0.20582670\n",
            "Iteration 1900, loss = 0.20570318\n",
            "Iteration 1901, loss = 0.20557996\n",
            "Iteration 1902, loss = 0.20545703\n",
            "Iteration 1903, loss = 0.20533440\n",
            "Iteration 1904, loss = 0.20521205\n",
            "Iteration 1905, loss = 0.20509005\n",
            "Iteration 1906, loss = 0.20496830\n",
            "Iteration 1907, loss = 0.20484686\n",
            "Iteration 1908, loss = 0.20472575\n",
            "Iteration 1909, loss = 0.20460485\n",
            "Iteration 1910, loss = 0.20448430\n",
            "Iteration 1911, loss = 0.20436415\n",
            "Iteration 1912, loss = 0.20424411\n",
            "Iteration 1913, loss = 0.20412448\n",
            "Iteration 1914, loss = 0.20400509\n",
            "Iteration 1915, loss = 0.20388606\n",
            "Iteration 1916, loss = 0.20376723\n",
            "Iteration 1917, loss = 0.20364872\n",
            "Iteration 1918, loss = 0.20353048\n",
            "Iteration 1919, loss = 0.20341252\n",
            "Iteration 1920, loss = 0.20329492\n",
            "Iteration 1921, loss = 0.20317758\n",
            "Iteration 1922, loss = 0.20306050\n",
            "Iteration 1923, loss = 0.20294364\n",
            "Iteration 1924, loss = 0.20282705\n",
            "Iteration 1925, loss = 0.20271090\n",
            "Iteration 1926, loss = 0.20259492\n",
            "Iteration 1927, loss = 0.20247918\n",
            "Iteration 1928, loss = 0.20236368\n",
            "Iteration 1929, loss = 0.20224864\n",
            "Iteration 1930, loss = 0.20213382\n",
            "Iteration 1931, loss = 0.20201918\n",
            "Iteration 1932, loss = 0.20190489\n",
            "Iteration 1933, loss = 0.20179096\n",
            "Iteration 1934, loss = 0.20167705\n",
            "Iteration 1935, loss = 0.20156360\n",
            "Iteration 1936, loss = 0.20145043\n",
            "Iteration 1937, loss = 0.20133747\n",
            "Iteration 1938, loss = 0.20122477\n",
            "Iteration 1939, loss = 0.20111235\n",
            "Iteration 1940, loss = 0.20100018\n",
            "Iteration 1941, loss = 0.20088827\n",
            "Iteration 1942, loss = 0.20077670\n",
            "Iteration 1943, loss = 0.20066536\n",
            "Iteration 1944, loss = 0.20055428\n",
            "Iteration 1945, loss = 0.20044350\n",
            "Iteration 1946, loss = 0.20033294\n",
            "Iteration 1947, loss = 0.20022267\n",
            "Iteration 1948, loss = 0.20011260\n",
            "Iteration 1949, loss = 0.20000279\n",
            "Iteration 1950, loss = 0.19989331\n",
            "Iteration 1951, loss = 0.19978406\n",
            "Iteration 1952, loss = 0.19967507\n",
            "Iteration 1953, loss = 0.19956633\n",
            "Iteration 1954, loss = 0.19945786\n",
            "Iteration 1955, loss = 0.19934963\n",
            "Iteration 1956, loss = 0.19924166\n",
            "Iteration 1957, loss = 0.19913396\n",
            "Iteration 1958, loss = 0.19902654\n",
            "Iteration 1959, loss = 0.19891931\n",
            "Iteration 1960, loss = 0.19881237\n",
            "Iteration 1961, loss = 0.19870563\n",
            "Iteration 1962, loss = 0.19859914\n",
            "Iteration 1963, loss = 0.19849295\n",
            "Iteration 1964, loss = 0.19838699\n",
            "Iteration 1965, loss = 0.19828140\n",
            "Iteration 1966, loss = 0.19817580\n",
            "Iteration 1967, loss = 0.19807063\n",
            "Iteration 1968, loss = 0.19796571\n",
            "Iteration 1969, loss = 0.19786097\n",
            "Iteration 1970, loss = 0.19775650\n",
            "Iteration 1971, loss = 0.19765220\n",
            "Iteration 1972, loss = 0.19754828\n",
            "Iteration 1973, loss = 0.19744450\n",
            "Iteration 1974, loss = 0.19734110\n",
            "Iteration 1975, loss = 0.19723782\n",
            "Iteration 1976, loss = 0.19713481\n",
            "Iteration 1977, loss = 0.19703197\n",
            "Iteration 1978, loss = 0.19692940\n",
            "Iteration 1979, loss = 0.19682713\n",
            "Iteration 1980, loss = 0.19672503\n",
            "Iteration 1981, loss = 0.19662328\n",
            "Iteration 1982, loss = 0.19652168\n",
            "Iteration 1983, loss = 0.19642036\n",
            "Iteration 1984, loss = 0.19631922\n",
            "Iteration 1985, loss = 0.19621834\n",
            "Iteration 1986, loss = 0.19611766\n",
            "Iteration 1987, loss = 0.19601722\n",
            "Iteration 1988, loss = 0.19591709\n",
            "Iteration 1989, loss = 0.19581710\n",
            "Iteration 1990, loss = 0.19571740\n",
            "Iteration 1991, loss = 0.19561788\n",
            "Iteration 1992, loss = 0.19551867\n",
            "Iteration 1993, loss = 0.19541960\n",
            "Iteration 1994, loss = 0.19532083\n",
            "Iteration 1995, loss = 0.19522226\n",
            "Iteration 1996, loss = 0.19512391\n",
            "Iteration 1997, loss = 0.19502570\n",
            "Iteration 1998, loss = 0.19492784\n",
            "Iteration 1999, loss = 0.19483018\n",
            "Training loss did not improve more than tol=0.000100 for 10 consecutive epochs. Stopping.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#scikit for machine learning reporting\n",
        "from sklearn.metrics import mean_squared_error\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "print(classification_report(y_test,y_pred)) # Print summary report\n",
        "print(confusion_matrix(y_test.argmax(axis=1), y_pred.argmax(axis=1))) # Print Confusion matrix\n",
        "print('accuracy is ',accuracy_score(y_pred,y_test)) # Print accuracy score"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nq8sT9UP9Mxv",
        "outputId": "96e292ca-0cf2-44d7-bec9-bb4c820b5091"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       1.00      1.00      1.00        13\n",
            "           1       0.89      0.94      0.92        18\n",
            "           2       1.00      0.93      0.96        14\n",
            "\n",
            "   micro avg       0.96      0.96      0.96        45\n",
            "   macro avg       0.96      0.96      0.96        45\n",
            "weighted avg       0.96      0.96      0.96        45\n",
            " samples avg       0.94      0.96      0.95        45\n",
            "\n",
            "[[13  0  0]\n",
            " [ 1 17  0]\n",
            " [ 0  2 12]]\n",
            "accuracy is  0.9333333333333333\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.9/dist-packages/sklearn/metrics/_classification.py:1344: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in samples with no predicted labels. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, msg_start, len(result))\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!cat '0.9333333333333333' > aaa.txt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MY6ZWDeeMhLQ",
        "outputId": "df771579-bb4b-4f10-9d4a-905667ecbb4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9333333333333333"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.plot(h.loss_curve_)\n",
        "plt.title('Loss History')\n",
        "plt.xlabel('epoch')\n",
        "plt.legend(['Loss'])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 312
        },
        "id": "ir9VVwZT9Rma",
        "outputId": "0cfe5de2-9264-40e8-856b-9dd27c167488"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.legend.Legend at 0x7f85880e5250>"
            ]
          },
          "metadata": {},
          "execution_count": 9
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C-EXQ3it9Uh7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}